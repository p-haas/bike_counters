{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import datetime\n",
    "import glob\n",
    "import io\n",
    "import re\n",
    "import warnings\n",
    "import zipfile\n",
    "\n",
    "# Third-Party Imports\n",
    "import branca.colormap as cm\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import holidays\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import pyod\n",
    "from pyod.models.xgbod import IForest\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import shapely\n",
    "from shapely import wkb\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from scipy.stats import kurtosis, mode, pearsonr, skew, t\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from skrub import TableVectorizer\n",
    "\n",
    "# Importing custom libraries\n",
    "from wip_lib import date_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import provided data\n",
    "train = pd.read_parquet(\"/Users/pierrehaas/bike_counters/data/train.parquet\")\n",
    "test = pd.read_parquet(\"/Users/pierrehaas/bike_counters/data/final_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additionally sourced data\n",
    "\n",
    "# https://meteo.data.gouv.fr/datasets/donnees-climatologiques-de-base-horaires/\n",
    "weather = pd.read_csv(\n",
    "    \"/Users/pierrehaas/bike_counters/external_data/weather/H_75_previous-2020-2022.csv.gz\",\n",
    "    parse_dates=[\"AAAAMMJJHH\"],\n",
    "    date_format=\"%Y%m%d%H\",\n",
    "    compression=\"gzip\",\n",
    "    sep=\";\",\n",
    ").rename(columns={\"AAAAMMJJHH\": \"date\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public transport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/information/\n",
    "# URLs of the zip files\n",
    "urls = [\n",
    "    \"https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/files/e6bcf4c994951fc086e31db6819a3448/download/\",\n",
    "    \"https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/files/e35b9ec0a183a8f2c7a8537dd43b124c/download/\",\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# File matching pattern\n",
    "pattern = r\"data-rf-202\\d/202\\d_S\\d+_NB_FER\\.txt\"\n",
    "\n",
    "# Process each ZIP file\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            # Get a list of all files in the archive and filter matching files\n",
    "            matching_files = [f for f in z.namelist() if re.match(pattern, f)]\n",
    "\n",
    "            # Read and concatenate the matching files\n",
    "            for file in matching_files:\n",
    "                with z.open(file) as f:\n",
    "                    # Assuming the files are tab-separated and have a \"JOUR\" column\n",
    "                    df = pd.read_csv(f, sep=\"\\t\", parse_dates=[\"JOUR\"], dayfirst=True)\n",
    "                    dfs.append(df)\n",
    "\n",
    "# Combine all DataFrames\n",
    "underground_transport = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "\n",
    "# # https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-surface/information/\n",
    "# # URLs of the zip files\n",
    "urls = [\n",
    "    \"https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-surface/files/41adcbd4216382c232ced4ccbf60187e/download/\",\n",
    "    \"https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-surface/files/68cac32e8717f476905a60006a4dca26/download/\",\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# File matching pattern\n",
    "pattern = r\"data-rs-202\\d/202\\d_T\\d+_NB_SURFACE\\.txt\"\n",
    "\n",
    "# Process each ZIP file\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            # Get a list of all files in the archive and filter matching files\n",
    "            matching_files = [f for f in z.namelist() if re.match(pattern, f)]\n",
    "\n",
    "            # Read and concatenate the matching files\n",
    "            for file in matching_files:\n",
    "                with z.open(file) as f:\n",
    "                    # Assuming the files are tab-separated and have a \"JOUR\" column\n",
    "                    df = pd.read_csv(\n",
    "                        f,\n",
    "                        sep=\"\\t\",\n",
    "                        parse_dates=[\"JOUR\"],\n",
    "                        dayfirst=True,\n",
    "                        encoding=\"latin1\",\n",
    "                    )\n",
    "                    dfs.append(df)\n",
    "\n",
    "# Combine all DataFrames\n",
    "overground_transport = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Car traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs of the zip files\n",
    "urls = [\n",
    "    \"https://parisdata.opendatasoft.com/api/datasets/1.0/comptages-routiers-permanents-historique/attachments/opendata_txt_2020_zip/\",\n",
    "    # \"https://parisdata.opendatasoft.com/api/datasets/1.0/comptages-routiers-permanents-historique/attachments/opendata_txt_2021_zip/\", # This file's compression format is broken, thus I provide a download link below\n",
    "    \"https://www.dropbox.com/scl/fi/sfqzlzpyxcf4yied3yucc/comptage-routier-2021.zip?rlkey=6k6hr3kywl8tvm4ax1qv2nv88&st=ktehiium&dl=1\",\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Process each ZIP file\n",
    "for i, url in enumerate(urls):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            # Process every file in the archive\n",
    "            for file in z.namelist():\n",
    "                # Skip directories and __MACOSX files\n",
    "                if file.endswith(\"/\") or \"__MACOSX\" in file:\n",
    "                    continue\n",
    "                # For the second URL, ensure files are within \"comptage-routier-2021\" directory\n",
    "                if i == 1 and not file.startswith(\"comptage-routier-2021/\"):\n",
    "                    continue\n",
    "                with z.open(file) as f:\n",
    "                    # Assuming the files are semicolon-separated and have a \"t_1h\" column\n",
    "                    df = pd.read_csv(f, sep=\";\", parse_dates=[\"t_1h\"])\n",
    "                    dfs.append(df)\n",
    "\n",
    "# Combine all DataFrames\n",
    "cars_count = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construction sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://opendata.paris.fr/explore/dataset/chantiers-a-paris-copie0/information/?disjunctive.cp_arrondissement&disjunctive.chantier_categorie&disjunctive.moa_principal&disjunctive.chantier_synthese&disjunctive.localisation_detail&disjunctive.localisation_stationnement\n",
    "construction_sites_2020 = pd.read_parquet(\n",
    "    \"/Users/pierrehaas/bike_counters/external_data/construction_sites/chantiers-a-paris-2020.parquet\"\n",
    ")\n",
    "\n",
    "# https://opendata.paris.fr/explore/embed/dataset/chantiers-a-paris-copie1/table/?disjunctive.cp_arrondissement&disjunctive.chantier_categorie&disjunctive.moa_principal&disjunctive.chantier_synthese&disjunctive.localisation_detail&disjunctive.localisation_stationnement\n",
    "construction_sites_2021 = pd.read_parquet(\n",
    "    \"/Users/pierrehaas/bike_counters/external_data/construction_sites/chantiers-a-paris-2021.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"date\"].min(), train[\"date\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"date\"].min(), test[\"date\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking installation dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"counter_installation_date\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring bike count through time and location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping of bike count by counter location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total bike count per location\n",
    "location_counts = (\n",
    "    train.groupby([\"latitude\", \"longitude\"])[\"bike_count\"].sum().reset_index()\n",
    ")\n",
    "\n",
    "# Create a colormap\n",
    "colormap = cm.linear.PuRd_09.scale(\n",
    "    location_counts[\"bike_count\"].min(), location_counts[\"bike_count\"].max()\n",
    ")\n",
    "\n",
    "# Create the map\n",
    "m = folium.Map(\n",
    "    location=train[[\"latitude\", \"longitude\"]].mean(axis=0).values, zoom_start=13\n",
    ")\n",
    "\n",
    "# Add markers to the map\n",
    "for _, row in location_counts.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row[\"latitude\"], row[\"longitude\"]],\n",
    "        radius=7,\n",
    "        color=colormap(row[\"bike_count\"]),\n",
    "        fill=True,\n",
    "        fill_color=\"black\",  # Fill center with black\n",
    "        fill_opacity=1,  # Ensure the fill is opaque\n",
    "        popup=f\"Bike Count: {row['bike_count']}\",\n",
    "    ).add_to(m)\n",
    "\n",
    "# Add the colormap to the map\n",
    "colormap.add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the further away from the city center, the less bike passage there is. Let's try to find a smart way of creating a new feature that represents this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    train.groupby(\"site_name\", observed=True)[\"bike_count\"]\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counter with most bikes registered is: Totem 73 boulevard de Sébastopol. Let's use the coordinates from this site to create a distance feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating new distance feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long_center = (\n",
    "    train[[\"latitude\", \"longitude\"]]\n",
    "    - mode(\n",
    "        train[train[\"site_name\"] == \"Totem 73 boulevard de Sébastopol\"][\n",
    "            [\"latitude\", \"longitude\"]\n",
    "        ]\n",
    "    )[0]\n",
    ")\n",
    "\n",
    "# We calculate the distance the previously identified site to all others\n",
    "# We call the result the distance to the center since the station is close to the center of Paris\n",
    "dist_center = np.linalg.norm(\n",
    "    lat_long_center,\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Correlation coefficient (Distance to station with most counts VS. Log bike count): {pearsonr(dist_center, train[\"log_bike_count\"])[0]:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dist_center, train[\"log_bike_count\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"distance_to_center\"] = dist_center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same process for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long_center = (\n",
    "    test[[\"latitude\", \"longitude\"]]\n",
    "    - mode(\n",
    "        test[test[\"site_name\"] == \"Totem 73 boulevard de Sébastopol\"][\n",
    "            [\"latitude\", \"longitude\"]\n",
    "        ]\n",
    "    )[0]\n",
    ")\n",
    "\n",
    "test[\"distance_to_center\"] = np.linalg.norm(\n",
    "    lat_long_center,\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating clusters of sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by counter_name and calculate the mean log_bike_count\n",
    "grouped_train = (\n",
    "    train.groupby(\"counter_name\", observed=True)[\"log_bike_count\"].mean().reset_index()\n",
    ")\n",
    "\n",
    "# Reshape the data for clustering\n",
    "X = grouped_train[[\"log_bike_count\"]]\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)  # You can change the number of clusters\n",
    "grouped_train[\"cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "# Sort clusters by mean log_bike_count and reassign cluster labels\n",
    "sorted_clusters = (\n",
    "    grouped_train.groupby(\"cluster\")[\"log_bike_count\"].mean().sort_values().index\n",
    ")\n",
    "cluster_mapping = {\n",
    "    old_label: new_label for new_label, old_label in enumerate(sorted_clusters)\n",
    "}\n",
    "grouped_train[\"cluster\"] = grouped_train[\"cluster\"].map(cluster_mapping)\n",
    "\n",
    "# Merge the cluster labels back to the original DataFrame\n",
    "train = train.merge(\n",
    "    grouped_train[[\"counter_name\", \"cluster\"]], on=\"counter_name\", how=\"left\"\n",
    ")\n",
    "test = test.merge(\n",
    "    grouped_train[[\"counter_name\", \"cluster\"]], on=\"counter_name\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time series of bike count by location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the DataFrame so each counter name has its own column\n",
    "pivot_df = (\n",
    "    train.groupby([pd.Grouper(key=\"date\", freq=\"W\"), \"site_name\"], observed=False)[\n",
    "        \"bike_count\"\n",
    "    ]\n",
    "    .sum()\n",
    "    .unstack()\n",
    ")\n",
    "\n",
    "# Plot the data\n",
    "pivot_df.plot(figsize=(12, 6))\n",
    "plt.title(\"Bike Count by Date\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Bike Count\")\n",
    "plt.legend(title=\"Site Name\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of bike count by location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=\"counter_name\", y=\"log_bike_count\", data=train)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Counter Name\")\n",
    "plt.ylabel(\"Log Bike Count\")\n",
    "plt.title(\"Boxplot of Log Bike Count by Counter Name\")\n",
    "\n",
    "# Hide the x-axis labels\n",
    "plt.xticks([])\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weekly bike count table by location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby([pd.Grouper(key=\"date\", freq=\"W\"), \"site_name\"], observed=False)[\n",
    "    \"bike_count\"\n",
    "].sum().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like some sites are not capturing any data at all at times (multiple weeks sometimes). We will need to investigate this further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a specific site that did not capture bikes for multiple weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Filter the data\n",
    "filtered_data = train[\n",
    "    (train[\"site_name\"] == \"20 Avenue de Clichy\") & (train[\"bike_count\"] == 0)\n",
    "][\"date\"].dt.hour\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(filtered_data, bins=24, kde=True, color=\"purple\")\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title(\n",
    "    \"Distribution of Hours with Zero Bike Count at 20 Avenue de Clichy\", fontsize=16\n",
    ")\n",
    "plt.xlabel(\"Hour of the Day\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "\n",
    "# Customize the ticks\n",
    "plt.xticks(range(0, 24), fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Add gridlines\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should try to find information about sites under repair (or else) to improve our model and make sure it predicts 0 bike_count when the site cannot capture data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if there are changes to some variables (e.g., change in counter id) when the bike count is 0 for a prolonged period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\n",
    "    (train[\"date\"] >= \"2021-05-09\")\n",
    "    & (train[\"date\"] <= \"2021-07-25\")\n",
    "    & (train[\"site_name\"] == \"20 Avenue de Clichy\")\n",
    "].groupby(\"counter_name\", observed=True).nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not seem to be a pattern of change in any variable when the bike count is 0 for a prolonged period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding dates for further analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded, test_encoded = date_encoder(train), date_encoder(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing impact of date features on bike count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "# Plot for 'is_bank_holiday'\n",
    "sns.boxplot(\n",
    "    ax=axes[0, 0], x=train_encoded[\"is_bank_holiday\"], y=train_encoded[\"log_bike_count\"]\n",
    ")\n",
    "plt.setp(axes[0, 0].artists, alpha=0.5)\n",
    "sns.violinplot(\n",
    "    ax=axes[0, 0], x=train_encoded[\"is_bank_holiday\"], y=train_encoded[\"log_bike_count\"]\n",
    ")\n",
    "plt.setp(axes[0, 0].collections, alpha=0.5)\n",
    "axes[0, 0].set_title(\"is_bank_holiday\")\n",
    "axes[0, 0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Plot for 'is_school_holiday'\n",
    "sns.boxplot(\n",
    "    ax=axes[0, 1],\n",
    "    x=train_encoded[\"is_school_holiday\"],\n",
    "    y=train_encoded[\"log_bike_count\"],\n",
    ")\n",
    "plt.setp(axes[0, 1].artists, alpha=0.5)\n",
    "sns.violinplot(\n",
    "    ax=axes[0, 1],\n",
    "    x=train_encoded[\"is_school_holiday\"],\n",
    "    y=train_encoded[\"log_bike_count\"],\n",
    ")\n",
    "plt.setp(axes[0, 1].collections, alpha=0.5)\n",
    "axes[0, 1].set_title(\"is_school_holiday\")\n",
    "axes[0, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Plot for 'is_weekend'\n",
    "sns.boxplot(\n",
    "    ax=axes[1, 0], x=train_encoded[\"is_weekend\"], y=train_encoded[\"log_bike_count\"]\n",
    ")\n",
    "plt.setp(axes[1, 0].artists, alpha=0.5)\n",
    "sns.violinplot(\n",
    "    ax=axes[1, 0], x=train_encoded[\"is_weekend\"], y=train_encoded[\"log_bike_count\"]\n",
    ")\n",
    "plt.setp(axes[1, 0].collections, alpha=0.5)\n",
    "axes[1, 0].set_title(\"is_weekend\")\n",
    "axes[1, 0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Plot for 'weekday'\n",
    "sns.boxplot(\n",
    "    ax=axes[1, 1], x=train_encoded[\"weekday\"], y=train_encoded[\"log_bike_count\"]\n",
    ")\n",
    "plt.setp(axes[1, 1].artists, alpha=0.5)\n",
    "sns.violinplot(\n",
    "    ax=axes[1, 1], x=train_encoded[\"weekday\"], y=train_encoded[\"log_bike_count\"]\n",
    ")\n",
    "plt.setp(axes[1, 1].collections, alpha=0.5)\n",
    "axes[1, 1].set_title(\"weekday\")\n",
    "axes[1, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Plot for 'month'\n",
    "sns.boxplot(ax=axes[2, 0], x=train_encoded[\"month\"], y=train_encoded[\"log_bike_count\"])\n",
    "plt.setp(axes[2, 0].artists, alpha=0.5)\n",
    "sns.violinplot(\n",
    "    ax=axes[2, 0], x=train_encoded[\"month\"], y=train_encoded[\"log_bike_count\"]\n",
    ")\n",
    "plt.setp(axes[2, 0].collections, alpha=0.5)\n",
    "axes[2, 0].set_title(\"month\")\n",
    "axes[2, 0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Plot for 'quarter'\n",
    "sns.boxplot(\n",
    "    ax=axes[2, 1], x=train_encoded[\"quarter\"], y=train_encoded[\"log_bike_count\"]\n",
    ")\n",
    "plt.setp(axes[2, 1].artists, alpha=0.5)\n",
    "sns.violinplot(\n",
    "    ax=axes[2, 1], x=train_encoded[\"quarter\"], y=train_encoded[\"log_bike_count\"]\n",
    ")\n",
    "plt.setp(axes[2, 1].collections, alpha=0.5)\n",
    "axes[2, 1].set_title(\"quarter\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plot = (\n",
    "    train_encoded.groupby([\"hour\", \"weekday\"])[\"log_bike_count\"]\n",
    "    .mean()\n",
    "    .unstack()\n",
    "    .plot(\n",
    "        figsize=(12, 8),  # Increase the figure size\n",
    "        colormap=\"viridis\",  # Use a colormap\n",
    "        title=\"Average Log Bike Count by Hour and Weekday\",  # Add a title\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add labels\n",
    "plot.set_xlabel(\"Hour\")\n",
    "plot.set_ylabel(\"Average Log Bike Count\")\n",
    "plot.set_title(\"Average Log Bike Count by Hour and Weekday\")\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plot = (\n",
    "    train_encoded.groupby([\"hour_scaled\", \"weekday\"])[\"log_bike_count\"]\n",
    "    .mean()\n",
    "    .unstack()\n",
    "    .plot(\n",
    "        figsize=(12, 8),  # Increase the figure size\n",
    "        colormap=\"viridis\",  # Use a colormap\n",
    "        title=\"Average Log Bike Count by Hour and Weekday\",  # Add a title\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add labels\n",
    "plot.set_xlabel(\"Hour\")\n",
    "plot.set_ylabel(\"Average Log Bike Count\")\n",
    "plot.set_title(\"Average Log Bike Count by Hour and Weekday\")\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather[\"date\"].min(), weather[\"date\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather[[\"NUM_POSTE\", \"NOM_USUEL\", \"LAT\", \"LON\", \"ALTI\"]].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.groupby(\"NUM_POSTE\")[\"NOM_USUEL\"].first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of weather stations in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(weather.groupby(\"date\")[\"NOM_USUEL\"].count(), linestyle=\"-\")\n",
    "plt.title(\"Number of data collection stations per Date\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Count of NOM_USUEL\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look at the location of these weather stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total count per location\n",
    "location_counts = (\n",
    "    weather.groupby([\"LAT\", \"LON\", \"NUM_POSTE\"]).size().reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# Create the map\n",
    "m = folium.Map(location=weather[[\"LAT\", \"LON\"]].mean(axis=0).values, zoom_start=13)\n",
    "\n",
    "# Add markers to the map\n",
    "for _, row in location_counts.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row[\"LAT\"], row[\"LON\"]],\n",
    "        radius=5,\n",
    "        color=\"black\",\n",
    "        fill=True,\n",
    "        fill_color=\"black\",\n",
    "        fill_opacity=1,\n",
    "        popup=f\"NUM_POSTE: {row['NUM_POSTE']}<br>Count: {row['count']}\",\n",
    "    ).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our view, there are two potential approaches to merging the weather data with the train and test sets:\n",
    "\n",
    "* Match the weather data to the train and test sets based on the closest weather station to the coordinates.\n",
    "* Match the weather data to the train and test sets using the average weather information from all collection stations in Paris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for missing values in the weather data. We shall drop the \"NUM_POSTE\", \"NOM_USUEL\", \"LAT\", \"LON\", \"ALTI\" columns since they are categorical (or just a marker of the specific data collection station)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.drop(\n",
    "    columns=[\"NUM_POSTE\", \"NOM_USUEL\", \"LAT\", \"LON\", \"ALTI\", \"date\"]\n",
    ").isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be fields with no values at all. We shall drop these fields and continue a more detailed on the remaining fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.drop(columns=[\"NUM_POSTE\", \"NOM_USUEL\", \"LAT\", \"LON\", \"ALTI\", \"date\"]).dropna(\n",
    "    axis=1, how=\"all\"\n",
    ").isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just went from 198 to 91 columns.\n",
    "\n",
    "Since there are a lot of missing values in the weather data, we will start by check if there are collection stations that have more missing values than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_per_num_poste = (\n",
    "    weather.drop(columns=[\"NOM_USUEL\", \"LAT\", \"LON\", \"ALTI\", \"date\"])\n",
    "    .dropna(axis=1, how=\"all\")\n",
    "    .groupby(\"NUM_POSTE\")\n",
    "    .apply(lambda x: x.isna().sum(), include_groups=False)\n",
    ")\n",
    "missing_values_per_num_poste.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we should use the 75114001 station only, or just average out the values from all stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, let's compute the average measure of each weather feature for each date in Paris as a whole (average out the measure over all stations at a given point in time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    weather.drop(columns=[\"NUM_POSTE\", \"NOM_USUEL\", \"LAT\", \"LON\", \"ALTI\"])\n",
    "    .dropna(axis=1, how=\"all\")\n",
    "    .groupby(\"date\")\n",
    "    .mean()\n",
    "    .isna()\n",
    "    .sum()\n",
    "    .sort_values()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"QDXI3S\" seems to have many missing values. We shall exclude it from the analysis. Now, let's check the remaining fields for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    weather.drop(columns=[\"NUM_POSTE\", \"NOM_USUEL\", \"LAT\", \"LON\", \"ALTI\", \"QDXI3S\"])\n",
    "    .groupby(\"date\")\n",
    "    .mean()\n",
    "    .dropna(axis=1, how=\"all\")\n",
    "    .isna()\n",
    "    .sum()\n",
    "    .sort_values()\n",
    "    != 0\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12 fields with missing values. We shall fill these missing values with linear interpolation since there are very few missing values. This is for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_reduced = (\n",
    "    weather.drop(columns=[\"NUM_POSTE\", \"NOM_USUEL\", \"LAT\", \"LON\", \"QDXI3S\"])\n",
    "    .groupby(\"date\")\n",
    "    .mean()\n",
    "    .dropna(axis=1, how=\"all\")\n",
    "    .interpolate(method=\"linear\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting weather data with feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_reduced[\"is_rain\"] = (weather_reduced[\"RR1\"] > 0).astype(int)\n",
    "\n",
    "weather_reduced[\"q_rain_lag_1h\"] = weather_reduced[\"RR1\"].shift(1)\n",
    "weather_reduced[\"t_rain_lag_1h\"] = weather_reduced[\"DRR1\"].shift(1)\n",
    "weather_reduced[\"is_rain_lag_1h\"] = weather_reduced[\"is_rain\"].shift(1)\n",
    "\n",
    "weather_reduced[\"q_rain_next_1h\"] = weather_reduced[\"RR1\"].shift(-1)\n",
    "weather_reduced[\"t_rain_next_1h\"] = weather_reduced[\"DRR1\"].shift(-1)\n",
    "weather_reduced[\"is_rain_next_1h\"] = weather_reduced[\"is_rain\"].shift(-1)\n",
    "\n",
    "weather_reduced[\"temp_lag_1h\"] = weather_reduced[\"T\"].shift(1)\n",
    "weather_reduced[\"temp_next_1h\"] = weather_reduced[\"T\"].shift(-1)\n",
    "\n",
    "weather_reduced[\"max_temp\"] = weather_reduced.groupby(weather_reduced.index.date)[\n",
    "    \"T\"\n",
    "].transform(\"max\")\n",
    "weather_reduced[\"will_rain\"] = (\n",
    "    weather_reduced.groupby(weather_reduced.index.date)[\"RR1\"]\n",
    "    .transform(lambda x: (x > 0).any())\n",
    "    .astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring weather relationship with bike count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_eda = (\n",
    "    train_encoded[[\"date\", \"counter_id\", \"log_bike_count\"]]\n",
    "    .merge(weather_reduced, left_on=\"date\", right_on=\"date\", how=\"left\")\n",
    "    .set_index([\"date\", \"counter_id\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation heatmap, uncovering linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = weather_eda.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(14, 14))\n",
    "\n",
    "# Draw the heatmap with the correlation matrix\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=False,\n",
    "    cmap=\"coolwarm\",\n",
    ")\n",
    "\n",
    "# Set the title\n",
    "plt.title(\"Correlation Matrix\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "\n",
    "high_corr_features = correlation_matrix[\n",
    "    np.abs(correlation_matrix[\"log_bike_count\"]) > threshold\n",
    "].index.values.tolist()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Draw the heatmap with the correlation matrix\n",
    "sns.heatmap(\n",
    "    correlation_matrix[np.abs(correlation_matrix[\"log_bike_count\"]) > threshold][\n",
    "        high_corr_features\n",
    "    ],\n",
    "    annot=False,\n",
    "    cmap=\"coolwarm\",\n",
    ")\n",
    "\n",
    "# Set the title\n",
    "plt.title(\"Correlation Matrix\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring relationships of selected features with log_bike_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_features = [\n",
    "    \"RR1\",\n",
    "    \"DRR1\",\n",
    "    \"T\",\n",
    "    \"TNSOL\",\n",
    "    \"TCHAUSSEE\",\n",
    "    \"U\",\n",
    "    \"GLO\",\n",
    "    \"is_rain\",\n",
    "    \"q_rain_lag_1h\",\n",
    "    \"t_rain_lag_1h\",\n",
    "    \"is_rain_lag_1h\",\n",
    "    \"q_rain_next_1h\",\n",
    "    \"t_rain_next_1h\",\n",
    "    \"is_rain_next_1h\",\n",
    "    \"temp_lag_1h\",\n",
    "    \"temp_next_1h\",\n",
    "    \"max_temp\",\n",
    "    \"will_rain\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_eda = (\n",
    "    train_encoded[[\"log_bike_count\", \"date\", \"hour\", \"weekday\", \"counter_id\"]]\n",
    "    .merge(\n",
    "        weather_reduced[weather_features], left_on=\"date\", right_on=\"date\", how=\"left\"\n",
    "    )\n",
    "    .set_index([\"date\", \"counter_id\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot for 'is_rain'\n",
    "sns.boxplot(ax=ax, x=weather_eda[\"is_rain\"], y=weather_eda[\"log_bike_count\"])\n",
    "plt.setp(ax.artists, alpha=0.5)\n",
    "sns.violinplot(ax=ax, x=weather_eda[\"is_rain\"], y=weather_eda[\"log_bike_count\"])\n",
    "plt.setp(ax.collections, alpha=0.5)\n",
    "ax.set_title(\"is_rain\")\n",
    "ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 12), sharey=True)\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# List of weekdays\n",
    "weekdays = [\n",
    "    \"Monday\",\n",
    "    \"Tuesday\",\n",
    "    \"Wednesday\",\n",
    "    \"Thursday\",\n",
    "    \"Friday\",\n",
    "    \"Saturday\",\n",
    "    \"Sunday\",\n",
    "]\n",
    "\n",
    "# Plot each weekday\n",
    "for i, weekday in enumerate(range(1, 8)):\n",
    "    ax = axes[i]\n",
    "    sns.boxplot(\n",
    "        ax=ax,\n",
    "        x=weather_eda[\"is_rain\"],\n",
    "        y=weather_eda[\"log_bike_count\"],\n",
    "        data=weather_eda[weather_eda[\"weekday\"] == weekday],\n",
    "    )\n",
    "    plt.setp(ax.artists, alpha=0.5)\n",
    "    sns.violinplot(\n",
    "        ax=ax,\n",
    "        x=weather_eda[\"is_rain\"],\n",
    "        y=weather_eda[\"log_bike_count\"],\n",
    "        data=weather_eda[weather_eda[\"weekday\"] == weekday],\n",
    "    )\n",
    "    plt.setp(ax.collections, alpha=0.5)\n",
    "    ax.set_title(weekdays[i])\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot for is_rain = 0\n",
    "sns.lineplot(\n",
    "    data=weather_eda[weather_eda[\"is_rain\"] == 0],\n",
    "    x=\"weekday\",\n",
    "    y=\"log_bike_count\",\n",
    "    label=\"No Rain\",\n",
    ")\n",
    "\n",
    "# Plot for is_rain = 1\n",
    "sns.lineplot(\n",
    "    data=weather_eda[weather_eda[\"is_rain\"] == 1],\n",
    "    x=\"weekday\",\n",
    "    y=\"log_bike_count\",\n",
    "    label=\"Rain\",\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Weekday\")\n",
    "plt.ylabel(\"Log Bike Count\")\n",
    "plt.title(\"Log Bike Count by Day and Rain Condition\")\n",
    "plt.legend(title=\"Rain Condition\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features\n",
    "num_features = len(weather_features)\n",
    "\n",
    "# Calculate the number of rows and columns for subplots\n",
    "nrows = (num_features + 1) // 2\n",
    "ncols = 2\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(15, 20))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each feature\n",
    "for i, feature in enumerate(weather_features):\n",
    "    axes[i].scatter(weather_eda[feature], weather_eda[\"log_bike_count\"])\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel(\"Log Bike Count\")\n",
    "    axes[i].set_title(f\"{feature} vs Log Bike Count\")\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = weather_eda.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Draw the heatmap with the correlation matrix\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=False,\n",
    "    cmap=\"coolwarm\",\n",
    ")\n",
    "\n",
    "# Set the title\n",
    "plt.title(\"Correlation Matrix\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing dimensionality of weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_features = [\n",
    "    \"RR1\",\n",
    "    \"DRR1\",\n",
    "    \"T\",\n",
    "    \"TNSOL\",\n",
    "    \"TCHAUSSEE\",\n",
    "    \"U\",\n",
    "    \"GLO\",\n",
    "    # \"is_rain\",\n",
    "    \"q_rain_lag_1h\",\n",
    "    \"t_rain_lag_1h\",\n",
    "    # \"is_rain_lag_1h\",\n",
    "    \"q_rain_next_1h\",\n",
    "    \"t_rain_next_1h\",\n",
    "    # \"is_rain_next_1h\",\n",
    "    \"temp_lag_1h\",\n",
    "    \"temp_next_1h\",\n",
    "    \"max_temp\",\n",
    "    # \"will_rain\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (weather_reduced.index >= train[\"date\"].min()) & (\n",
    "    weather_reduced.index <= test[\"date\"].max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA with the maximum number of components\n",
    "pca = PCA()\n",
    "pca.fit(weather_reduced[mask][weather_features])\n",
    "\n",
    "# Calculate the cumulative explained variance ratio\n",
    "cumulative_explained_variance = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# Plot the cumulative explained variance ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    range(1, len(cumulative_explained_variance) + 1),\n",
    "    cumulative_explained_variance,\n",
    "    marker=\"o\",\n",
    "    linestyle=\"--\",\n",
    "    color=\"b\",\n",
    ")\n",
    "plt.title(\"Explained Variance by Number of PCA Components\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "\n",
    "pca = PCA(n_components=n)\n",
    "\n",
    "X_pca1 = pca.fit_transform(weather_reduced[mask][weather_features])\n",
    "\n",
    "weather_pca = pd.DataFrame(\n",
    "    X_pca1,\n",
    "    index=weather_reduced[mask][weather_features].index,\n",
    "    columns=[\"weather_\" + str(i) for i in range(1, n + 1)],\n",
    ").reset_index()\n",
    "\n",
    "print(\n",
    "    f\"% of data variance explained by reduced PCA with {n} components: {pca.explained_variance_ratio_.sum():.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging weather data with train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose to ways of merging the weather data with the train and test sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Merge handpicked weather features with the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_encoded = train_encoded.merge(\n",
    "#     weather_reduced[weather_features],\n",
    "#     left_on=\"date\",\n",
    "#     right_index=True,\n",
    "#     how=\"left\",\n",
    "# )\n",
    "\n",
    "# test_encoded = test_encoded.merge(\n",
    "#     weather_reduced[weather_features],\n",
    "#     left_on=\"date\",\n",
    "#     right_index=True,\n",
    "#     how=\"left\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Merge the PCA-transformed weather data with the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded = train_encoded.merge(\n",
    "    weather_pca,\n",
    "    on=\"date\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "test_encoded = test_encoded.merge(\n",
    "    weather_pca,\n",
    "    on=\"date\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Public transport data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the total number of commuters per day in Ile-de-France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_underground_transport = underground_transport.groupby(\"JOUR\")[\"NB_VALD\"].sum()\n",
    "daily_underground_transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_overground_transport = overground_transport.groupby(\"JOUR\")[\"NB_VALD\"].sum()\n",
    "daily_overground_transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily totals\n",
    "daily_underground_transport = underground_transport.groupby(\"JOUR\")[\"NB_VALD\"].sum()\n",
    "daily_overground_transport = overground_transport.groupby(\"JOUR\")[\"NB_VALD\"].sum()\n",
    "\n",
    "# Resample to weekly totals\n",
    "weekly_underground_transport = daily_underground_transport.resample(\"W\").sum()\n",
    "weekly_overground_transport = daily_overground_transport.resample(\"W\").sum()\n",
    "\n",
    "# Plot daily totals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(daily_underground_transport, label=\"Underground\")\n",
    "plt.plot(daily_overground_transport, label=\"Overground\")\n",
    "plt.plot(daily_underground_transport + daily_overground_transport, label=\"Total\")\n",
    "plt.legend()\n",
    "plt.title(\"Daily Transport Totals\")\n",
    "\n",
    "# Plot weekly totals\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(weekly_underground_transport, label=\"Underground\")\n",
    "plt.plot(weekly_overground_transport, label=\"Overground\")\n",
    "plt.plot(weekly_underground_transport + weekly_overground_transport, label=\"Total\")\n",
    "plt.legend()\n",
    "plt.title(\"Weekly Transport Totals\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine the daily number of commuters of both overground and underground transport and encode the dates to push further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_transport = (\n",
    "    daily_underground_transport + daily_overground_transport\n",
    ").reset_index()\n",
    "\n",
    "daily_transport = date_encoder(daily_transport, col=\"JOUR\")[\n",
    "    (daily_transport[\"JOUR\"] >= \"2020-09-01\")\n",
    "    & (daily_transport[\"JOUR\"] <= \"2021-10-18\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_transport.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this new data, we will try to detect days of strikes in the public transport system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(daily_transport[\"NB_VALD\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fancy boxplot\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Plot boxplot for 'NB_VALD'\n",
    "sns.boxplot(ax=ax, y=daily_transport[\"NB_VALD\"])\n",
    "plt.setp(ax.artists, alpha=0.5)\n",
    "\n",
    "# Optionally, add a violin plot for additional visualization\n",
    "sns.violinplot(ax=ax, y=daily_transport[\"NB_VALD\"])\n",
    "plt.setp(ax.collections, alpha=0.5)\n",
    "\n",
    "ax.set_title(\"Boxplot of NB_VALD\", fontsize=16)\n",
    "ax.set_xlabel(\"NB_VALD\", fontsize=14)\n",
    "ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fancy boxplot\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Plot boxplot for 'NB_VALD'\n",
    "sns.boxplot(ax=ax, y=daily_transport[\"NB_VALD\"])\n",
    "plt.setp(ax.artists, alpha=0.5)\n",
    "\n",
    "# Optionally, add a violin plot for additional visualization\n",
    "sns.violinplot(ax=ax, y=daily_transport[\"NB_VALD\"])\n",
    "plt.setp(ax.collections, alpha=0.5)\n",
    "\n",
    "ax.set_title(\"Boxplot of NB_VALD\", fontsize=16)\n",
    "ax.set_xlabel(\"NB_VALD\", fontsize=14)\n",
    "ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_daily_transport = daily_transport[\"NB_VALD\"].mean()\n",
    "std_daily_transport = daily_transport[\"NB_VALD\"].std()\n",
    "mask = (daily_transport[\"NB_VALD\"] < avg_daily_transport - 2 * std_daily_transport) | (\n",
    "    daily_transport[\"NB_VALD\"] > avg_daily_transport + 2 * std_daily_transport\n",
    ")\n",
    "\n",
    "daily_transport[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging public transport data with train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded = train_encoded.merge(\n",
    "    daily_transport[[\"year\", \"month\", \"day\", \"NB_VALD\"]],\n",
    "    left_on=[\"year\", \"month\", \"day\"],\n",
    "    right_on=[\"year\", \"month\", \"day\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "test_encoded = test_encoded.merge(\n",
    "    daily_transport[[\"year\", \"month\", \"day\", \"NB_VALD\"]],\n",
    "    left_on=[\"year\", \"month\", \"day\"],\n",
    "    right_on=[\"year\", \"month\", \"day\"],\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Car traffic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_cars_count = (\n",
    "    cars_count[\n",
    "        (cars_count[\"t_1h\"] >= train[\"date\"].min() - datetime.timedelta(hours=1))\n",
    "        & (cars_count[\"t_1h\"] <= test[\"date\"].max() + datetime.timedelta(hours=1))\n",
    "    ]\n",
    "    .groupby(\"t_1h\")[\"q\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "hourly_cars_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by day and calculate the cumulative sum of 'q' for each day\n",
    "hourly_cars_count[\"daily_cumsum\"] = hourly_cars_count.groupby(\n",
    "    hourly_cars_count[\"t_1h\"].dt.to_period(\"d\")\n",
    ")[\"q\"].cumsum()\n",
    "hourly_cars_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by week and sum the car counts for each week\n",
    "weekly_sum = (\n",
    "    cars_count.groupby(cars_count[\"t_1h\"].dt.to_period(\"W\"))[\"q\"].sum().reset_index()\n",
    ")\n",
    "\n",
    "# Convert the period to datetime for plotting\n",
    "weekly_sum[\"t_1h\"] = weekly_sum[\"t_1h\"].dt.start_time\n",
    "\n",
    "# Plot the weekly average number of cars\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(ax=ax, x=weekly_sum[\"t_1h\"], y=weekly_sum[\"q\"], marker=\"o\")\n",
    "ax.set_title(\"Weekly Average Number of Cars\", fontsize=16)\n",
    "ax.set_xlabel(\"Week\", fontsize=14)\n",
    "ax.set_ylabel(\"Average Number of Cars\", fontsize=14)\n",
    "ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging car traffic data with train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded = train_encoded.merge(\n",
    "    hourly_cars_count, left_on=\"date\", right_on=\"t_1h\", how=\"left\"\n",
    ")\n",
    "test_encoded = test_encoded.merge(\n",
    "    hourly_cars_count, left_on=\"date\", right_on=\"t_1h\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construction_sites_2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construction_sites_2020.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construction_sites_2021.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construction_sites_2021.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "construction_sites = pd.concat(\n",
    "    [construction_sites_2020, construction_sites_2021], axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construction_sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construction_sites.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construction_sites.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construction_sites[\n",
    "    [\n",
    "        \"num_emprise\",\n",
    "        \"moa_principal\",\n",
    "        \"demande_cite_id\",\n",
    "        \"chantier_cite_id\",\n",
    "        \"geo_shape\",\n",
    "        \"geo_point_2d\",\n",
    "    ]\n",
    "].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding dates to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction_sites[\"date_debut\"] = pd.to_datetime(\n",
    "#     construction_sites[\"date_debut\"], format=\"%Y-%m-%d\"\n",
    "# )\n",
    "\n",
    "# construction_sites[\"date_fin\"] = pd.to_datetime(\n",
    "#     construction_sites[\"date_fin\"], format=\"%Y-%m-%d\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction_sites.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting duplicate construction sites and sites with no location information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to understand the data better by checking observations with duplicated chantier_cite_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construction_sites[\n",
    "    construction_sites[\"chantier_cite_id\"].duplicated(keep=False)\n",
    "].sort_values(\"chantier_cite_id\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the duplicated observations are not exactly the same. Looking at the first two observations, we can see that, although the chantier_cite_id are the same for the two, the observtions have different areas. We shall keep the two observations, and for that matter any other duplicated observation under the same scenario, and only drop duplicated observations that have the exact same information (it implies that the construction started in 2020 or before and ended in 2021 or after | observations were both in the 2020 and 2021 datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "construction_sites = construction_sites.drop_duplicates().dropna(\n",
    "    subset=[\"geo_shape\", \"geo_point_2d\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construction_sites.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_duplicates = construction_sites.duplicated(keep=False).any()\n",
    "print(f\"Are there duplicates? {has_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicates = construction_sites.duplicated(keep=False).sum()\n",
    "print(f\"Number of duplicated rows: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting relevant construction sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the types of construction sites in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construction_sites[\"localisation_detail\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there is a special encoding for construction sites that are located ob bike lanes (PISTE_CYCLABLE). We shall take a look at these observations specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction_sites_simplified = construction_sites[\n",
    "#     ~construction_sites[\"localisation_detail\"].isin(\n",
    "#         [\n",
    "#             \"EMPRISE_CHAUSSEE\",\n",
    "#             \"EMPRISE_TROTTOIR\",\n",
    "#             \"EMPRISE_CHAUSSEE,EMPRISE_TROTTOIR\",\n",
    "#             None,\n",
    "#         ]\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "\n",
    "# construction_sites_simplified[\"localisation_detail\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop irrelevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_reduced = construction_sites.drop(\n",
    "    columns=[\n",
    "        \"moa_principal\",\n",
    "        \"demande_cite_id\",\n",
    "        \"chantier_synthese\",\n",
    "        \"localisation_stationnement\",\n",
    "        \"chantier_cite_id\",\n",
    "        \"chantier_categorie\",\n",
    "        \"cp_arrondissement\",\n",
    "        \"surface\",\n",
    "        \"localisation_detail\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "cs_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing construction sites shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_point_wkb = cs_reduced[\"geo_shape\"].iloc[0]\n",
    "geometry = wkb.loads(geo_point_wkb)\n",
    "print(geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x and y coordinates for plotting\n",
    "x, y = geometry.exterior.xy\n",
    "\n",
    "# Plot the polygon\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Polygon Visualization\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming geoshapes to coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_reduced[[\"geo_shape\", \"geo_point_2d\"]] = cs_reduced[\n",
    "    [\"geo_shape\", \"geo_point_2d\"]\n",
    "].apply(lambda x: shapely.wkb.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count = cs_reduced[\"geo_shape\"].isnull().sum()\n",
    "if null_count > 0:\n",
    "    print(f\"There are {null_count} null geometries.\")\n",
    "else:\n",
    "    print(\"No null geometries found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count = cs_reduced[\"geo_point_2d\"].isnull().sum()\n",
    "if null_count > 0:\n",
    "    print(f\"There are {null_count} null geometries.\")\n",
    "else:\n",
    "    print(\"No null geometries found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(cs_reduced, geometry=\"geo_shape\", crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf.explore(\"area\", legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding if bike counters fall within construction sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_combined = pd.concat(\n",
    "    [train_encoded[\"latitude\"], test_encoded[\"latitude\"]], ignore_index=True\n",
    ")\n",
    "longitude_combined = pd.concat(\n",
    "    [train_encoded[\"longitude\"], test_encoded[\"longitude\"]], ignore_index=True\n",
    ")\n",
    "coords = list(\n",
    "    zip(\n",
    "        latitude_combined,\n",
    "        longitude_combined,\n",
    "    )\n",
    ")\n",
    "\n",
    "index1 = pd.concat([train_encoded[\"date\"], test_encoded[\"date\"]], ignore_index=True)\n",
    "index2 = pd.concat(\n",
    "    [train_encoded[\"counter_id\"], test_encoded[\"counter_id\"]], ignore_index=True\n",
    ")\n",
    "\n",
    "points_gdf = gpd.GeoDataFrame(\n",
    "    coords,\n",
    "    index=[index1, index2],\n",
    "    columns=[\"longitude\", \"latitude\"],\n",
    "    geometry=[Point(xy) for xy in coords],\n",
    "    crs=gdf.crs,  # Make sure this matches the gdf of construction sites\n",
    ")\n",
    "\n",
    "points_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join with predicate='within' to see which points lie inside a polygon\n",
    "joined = gpd.sjoin(points_gdf, gdf, how=\"left\", predicate=\"within\")\n",
    "\n",
    "joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Number of bike counters that fall (at some point) in a construction site: {joined.shape[0] - joined[\"num_emprise\"].isna().sum()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no bike counters that fall within construction sites. Thus, the construction site dataset will not be useful for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting outliers and potential strikes in public transport data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underground transport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using z-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At station level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut_grouped = (\n",
    "    underground_transport.groupby([\"JOUR\", \"CODE_STIF_ARRET\"])[\"NB_VALD\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "ut_grouped = ut_grouped[\n",
    "    (ut_grouped[\"JOUR\"] >= \"2020-09-01\") & (ut_grouped[\"JOUR\"] <= \"2021-10-18\")\n",
    "]\n",
    "\n",
    "# Ensure JOUR is a datetime type\n",
    "ut_grouped[\"JOUR\"] = pd.to_datetime(ut_grouped[\"JOUR\"])\n",
    "\n",
    "# Extract the day of week (0=Monday, 6=Sunday)\n",
    "ut_grouped[\"day_of_week\"] = ut_grouped[\"JOUR\"].dt.dayofweek\n",
    "\n",
    "# Compute baseline stats (mean and std) per day_of_week\n",
    "baseline_stats = (\n",
    "    ut_grouped.groupby([\"CODE_STIF_ARRET\", \"day_of_week\"])[\"NB_VALD\"]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "baseline_stats.rename(columns={\"mean\": \"mean_val\", \"std\": \"std_val\"}, inplace=True)\n",
    "\n",
    "# Merge the baseline stats back to the original DataFrame\n",
    "ut_grouped = ut_grouped.merge(\n",
    "    baseline_stats, on=[\"CODE_STIF_ARRET\", \"day_of_week\"], how=\"left\"\n",
    ")\n",
    "\n",
    "# Compute z-score for each day’s NB_VALD\n",
    "# Handle cases where std is zero by replacing with a small number\n",
    "ut_grouped[\"std_val\"] = ut_grouped[\"std_val\"].replace(0, np.nan)\n",
    "ut_grouped[\"z_score\"] = (ut_grouped[\"NB_VALD\"] - ut_grouped[\"mean_val\"]) / ut_grouped[\n",
    "    \"std_val\"\n",
    "]\n",
    "\n",
    "# Define an anomaly threshold\n",
    "# For example, we consider a strike if it's 3 standard deviations below the mean\n",
    "threshold = -3\n",
    "\n",
    "# Flag potential strike days\n",
    "ut_grouped[\"potential_strike\"] = ut_grouped[\"z_score\"] < threshold\n",
    "\n",
    "# Optionally, select only those rows that are flagged\n",
    "possible_strike_days = ut_grouped[ut_grouped[\"potential_strike\"]]\n",
    "\n",
    "print(\"Potential strike days:\")\n",
    "possible_strike_days[\n",
    "    [\"JOUR\", \"CODE_STIF_ARRET\", \"NB_VALD\", \"z_score\", \"potential_strike\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_strike_days.groupby(\"JOUR\")[\"potential_strike\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At daily level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut_grouped = underground_transport.groupby([\"JOUR\"])[\"NB_VALD\"].sum().reset_index()\n",
    "ut_grouped = ut_grouped[\n",
    "    (ut_grouped[\"JOUR\"] >= \"2020-09-01\") & (ut_grouped[\"JOUR\"] <= \"2021-10-18\")\n",
    "]\n",
    "\n",
    "# Ensure JOUR is a datetime type\n",
    "ut_grouped[\"JOUR\"] = pd.to_datetime(ut_grouped[\"JOUR\"])\n",
    "\n",
    "# Extract the day of week (0=Monday, 6=Sunday)\n",
    "ut_grouped[\"day_of_week\"] = ut_grouped[\"JOUR\"].dt.dayofweek\n",
    "\n",
    "# Compute baseline stats (mean and std) per day_of_week\n",
    "baseline_stats = (\n",
    "    ut_grouped.groupby([\"day_of_week\"])[\"NB_VALD\"].agg([\"mean\", \"std\"]).reset_index()\n",
    ")\n",
    "baseline_stats.rename(columns={\"mean\": \"mean_val\", \"std\": \"std_val\"}, inplace=True)\n",
    "\n",
    "# Merge the baseline stats back to the original DataFrame\n",
    "ut_grouped = ut_grouped.merge(baseline_stats, on=[\"day_of_week\"], how=\"left\")\n",
    "\n",
    "# Compute z-score for each day’s NB_VALD\n",
    "# Handle cases where std is zero by replacing with a small number\n",
    "ut_grouped[\"std_val\"] = ut_grouped[\"std_val\"].replace(0, np.nan)\n",
    "ut_grouped[\"z_score\"] = (ut_grouped[\"NB_VALD\"] - ut_grouped[\"mean_val\"]) / ut_grouped[\n",
    "    \"std_val\"\n",
    "]\n",
    "\n",
    "# Define an anomaly threshold\n",
    "# For example, we consider a strike if it's 3 standard deviations below the mean\n",
    "threshold = -1.5\n",
    "\n",
    "# Flag potential strike days\n",
    "ut_grouped[\"potential_strike\"] = ut_grouped[\"z_score\"] < threshold\n",
    "\n",
    "# Optionally, select only those rows that are flagged\n",
    "possible_strike_days = ut_grouped[ut_grouped[\"potential_strike\"]]\n",
    "\n",
    "print(\"Potential strike days:\")\n",
    "possible_strike_days[[\"JOUR\", \"NB_VALD\", \"z_score\", \"potential_strike\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_strike_days.groupby(\"JOUR\")[\"potential_strike\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using t-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At station level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by day and station, sum NB_VALD\n",
    "ut_grouped = (\n",
    "    underground_transport.groupby([\"JOUR\", \"CODE_STIF_ARRET\"])[\"NB_VALD\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "ut_grouped = ut_grouped[\n",
    "    (ut_grouped[\"JOUR\"] >= \"2020-09-01\") & (ut_grouped[\"JOUR\"] <= \"2021-10-18\")\n",
    "]\n",
    "\n",
    "# Ensure JOUR is a datetime type\n",
    "ut_grouped[\"JOUR\"] = pd.to_datetime(ut_grouped[\"JOUR\"])\n",
    "\n",
    "# Extract the day of week (0=Monday, 6=Sunday)\n",
    "ut_grouped[\"day_of_week\"] = ut_grouped[\"JOUR\"].dt.dayofweek\n",
    "\n",
    "# Compute baseline stats (mean, std, and count) per day_of_week and station\n",
    "baseline_stats = (\n",
    "    ut_grouped.groupby([\"CODE_STIF_ARRET\", \"day_of_week\"])[\"NB_VALD\"]\n",
    "    .agg([\"mean\", \"std\", \"count\"])\n",
    "    .reset_index()\n",
    ")\n",
    "baseline_stats.rename(\n",
    "    columns={\"mean\": \"mean_val\", \"std\": \"std_val\", \"count\": \"n\"}, inplace=True\n",
    ")\n",
    "\n",
    "# Merge the baseline stats back to the original DataFrame\n",
    "ut_grouped = ut_grouped.merge(\n",
    "    baseline_stats, on=[\"CODE_STIF_ARRET\", \"day_of_week\"], how=\"left\"\n",
    ")\n",
    "\n",
    "# Compute studentized residuals for each day’s NB_VALD\n",
    "ut_grouped[\"std_val\"] = ut_grouped[\"std_val\"].replace(0, np.nan)\n",
    "ut_grouped[\"studentized_residual\"] = (\n",
    "    (ut_grouped[\"NB_VALD\"] - ut_grouped[\"mean_val\"]) / ut_grouped[\"std_val\"]\n",
    ") * np.sqrt((ut_grouped[\"n\"] + 1) / ut_grouped[\"n\"])\n",
    "\n",
    "# Define an anomaly threshold using the t-distribution\n",
    "threshold = t.ppf(\n",
    "    0.025, df=ut_grouped[\"n\"] - 1\n",
    ")  # 2.5% significance level for two-tailed test\n",
    "\n",
    "# Flag potential strike days\n",
    "ut_grouped[\"potential_strike\"] = ut_grouped[\"studentized_residual\"] < threshold\n",
    "\n",
    "# Optionally, select only those rows that are flagged\n",
    "possible_strike_days = ut_grouped[ut_grouped[\"potential_strike\"]]\n",
    "\n",
    "print(\"Potential strike days:\")\n",
    "possible_strike_days[\n",
    "    [\"JOUR\", \"CODE_STIF_ARRET\", \"NB_VALD\", \"studentized_residual\", \"potential_strike\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_strike_days.groupby(\"JOUR\")[\"potential_strike\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At daily level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by day and sum NB_VALD\n",
    "ut_grouped = underground_transport.groupby([\"JOUR\"])[\"NB_VALD\"].sum().reset_index()\n",
    "ut_grouped = ut_grouped[\n",
    "    (ut_grouped[\"JOUR\"] >= \"2020-09-01\") & (ut_grouped[\"JOUR\"] <= \"2021-10-18\")\n",
    "]\n",
    "\n",
    "# Ensure JOUR is a datetime type\n",
    "ut_grouped[\"JOUR\"] = pd.to_datetime(ut_grouped[\"JOUR\"])\n",
    "\n",
    "# Extract the day of week (0=Monday, 6=Sunday)\n",
    "ut_grouped[\"day_of_week\"] = ut_grouped[\"JOUR\"].dt.dayofweek\n",
    "\n",
    "# Compute baseline stats (mean and std) per day_of_week\n",
    "baseline_stats = (\n",
    "    ut_grouped.groupby([\"day_of_week\"])[\"NB_VALD\"]\n",
    "    .agg([\"mean\", \"std\", \"count\"])\n",
    "    .reset_index()\n",
    ")\n",
    "baseline_stats.rename(\n",
    "    columns={\"mean\": \"mean_val\", \"std\": \"std_val\", \"count\": \"n\"}, inplace=True\n",
    ")\n",
    "\n",
    "# Merge the baseline stats back to the original DataFrame\n",
    "ut_grouped = ut_grouped.merge(baseline_stats, on=[\"day_of_week\"], how=\"left\")\n",
    "\n",
    "# Compute studentized residuals for each day’s NB_VALD\n",
    "ut_grouped[\"std_val\"] = ut_grouped[\"std_val\"].replace(0, np.nan)\n",
    "ut_grouped[\"studentized_residual\"] = (\n",
    "    (ut_grouped[\"NB_VALD\"] - ut_grouped[\"mean_val\"]) / ut_grouped[\"std_val\"]\n",
    ") * np.sqrt((ut_grouped[\"n\"] + 1) / ut_grouped[\"n\"])\n",
    "\n",
    "# Define an anomaly threshold using the t-distribution\n",
    "threshold = t.ppf(\n",
    "    0.025, df=ut_grouped[\"n\"] - 1\n",
    ")  # 2.5% significance level for two-tailed test\n",
    "\n",
    "# Flag potential strike days\n",
    "ut_grouped[\"potential_strike\"] = ut_grouped[\"studentized_residual\"] < threshold\n",
    "\n",
    "# Optionally, select only those rows that are flagged\n",
    "possible_strike_days = ut_grouped[ut_grouped[\"potential_strike\"]]\n",
    "\n",
    "print(\"Potential strike days:\")\n",
    "possible_strike_days[[\"JOUR\", \"NB_VALD\", \"studentized_residual\", \"potential_strike\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_strike_days.groupby(\"JOUR\")[\"potential_strike\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pyod library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "underground_transport.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "underground_transport.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ut_grouped = underground_transport.groupby([\"JOUR\"])[\"NB_VALD\"].sum().reset_index()\n",
    "# ut_grouped = date_encoder(ut_grouped, col=\"JOUR\")\n",
    "# ut_grouped = ut_grouped[\n",
    "#     (ut_grouped[\"JOUR\"] >= \"2020-09-01\")\n",
    "#     & (ut_grouped[\"JOUR\"] <= \"2021-10-18\")\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = TableVectorizer()\n",
    "# X_encoded = encoder.fit_transform(ut_grouped.drop(columns=[\"JOUR\"]))\n",
    "# X_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = IForest()\n",
    "# model.fit(X_encoded)\n",
    "# y_pred = model.predict(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ut_grouped[\"outliers\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ut_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'outliers' DataFrame has a 'group' column for grouping and 'NB_VALD' column for values\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.boxplot(data=ut_grouped, x='outliers', y='NB_VALD')\n",
    "# plt.title(\"Boxplot of NB_VALD by Group\", fontsize=16)\n",
    "# plt.xlabel(\"Group\", fontsize=14)\n",
    "# plt.ylabel(\"NB_VALD\", fontsize=14)\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.grid(True, linestyle='--', alpha=0.7)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overground transport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using z-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At station level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot_grouped = (\n",
    "    overground_transport.groupby([\"JOUR\", \"CODE_STIF_LIGNE\"])[\"NB_VALD\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "ot_grouped = ot_grouped[\n",
    "    (ot_grouped[\"JOUR\"] >= \"2020-09-01\") & (ot_grouped[\"JOUR\"] <= \"2021-10-18\")\n",
    "]\n",
    "\n",
    "# Ensure JOUR is a datetime type\n",
    "ot_grouped[\"JOUR\"] = pd.to_datetime(ot_grouped[\"JOUR\"])\n",
    "\n",
    "# Extract the day of week (0=Monday, 6=Sunday)\n",
    "ot_grouped[\"day_of_week\"] = ot_grouped[\"JOUR\"].dt.dayofweek\n",
    "\n",
    "# Compute baseline stats (mean and std) per day_of_week\n",
    "baseline_stats = (\n",
    "    ot_grouped.groupby([\"CODE_STIF_LIGNE\", \"day_of_week\"])[\"NB_VALD\"]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "baseline_stats.rename(columns={\"mean\": \"mean_val\", \"std\": \"std_val\"}, inplace=True)\n",
    "\n",
    "# Merge the baseline stats back to the original DataFrame\n",
    "ot_grouped = ot_grouped.merge(\n",
    "    baseline_stats, on=[\"CODE_STIF_LIGNE\", \"day_of_week\"], how=\"left\"\n",
    ")\n",
    "\n",
    "# Compute z-score for each day’s NB_VALD\n",
    "# Handle cases where std is zero by replacing with a small number\n",
    "ot_grouped[\"std_val\"] = ot_grouped[\"std_val\"].replace(0, np.nan)\n",
    "ot_grouped[\"z_score\"] = (ot_grouped[\"NB_VALD\"] - ot_grouped[\"mean_val\"]) / ot_grouped[\n",
    "    \"std_val\"\n",
    "]\n",
    "\n",
    "# Define an anomaly threshold\n",
    "# For example, we consider a strike if it's 3 standard deviations below the mean\n",
    "threshold = -3\n",
    "\n",
    "# Flag potential strike days\n",
    "ot_grouped[\"potential_strike\"] = ot_grouped[\"z_score\"] < threshold\n",
    "\n",
    "# Optionally, select only those rows that are flagged\n",
    "possible_strike_days = ot_grouped[ot_grouped[\"potential_strike\"]]\n",
    "\n",
    "print(\"Potential strike days:\")\n",
    "possible_strike_days[\n",
    "    [\"JOUR\", \"CODE_STIF_LIGNE\", \"NB_VALD\", \"z_score\", \"potential_strike\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_strike_days.groupby(\"JOUR\")[\"potential_strike\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At daily level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot_grouped = overground_transport.groupby([\"JOUR\"])[\"NB_VALD\"].sum().reset_index()\n",
    "ot_grouped = ot_grouped[\n",
    "    (ot_grouped[\"JOUR\"] >= \"2020-09-01\") & (ot_grouped[\"JOUR\"] <= \"2021-10-18\")\n",
    "]\n",
    "\n",
    "# Ensure JOUR is a datetime type\n",
    "ot_grouped[\"JOUR\"] = pd.to_datetime(ot_grouped[\"JOUR\"])\n",
    "\n",
    "# Extract the day of week (0=Monday, 6=Sunday)\n",
    "ot_grouped[\"day_of_week\"] = ot_grouped[\"JOUR\"].dt.dayofweek\n",
    "\n",
    "# Compute baseline stats (mean and std) per day_of_week\n",
    "baseline_stats = (\n",
    "    ot_grouped.groupby([\"day_of_week\"])[\"NB_VALD\"].agg([\"mean\", \"std\"]).reset_index()\n",
    ")\n",
    "baseline_stats.rename(columns={\"mean\": \"mean_val\", \"std\": \"std_val\"}, inplace=True)\n",
    "\n",
    "# Merge the baseline stats back to the original DataFrame\n",
    "ot_grouped = ot_grouped.merge(baseline_stats, on=[\"day_of_week\"], how=\"left\")\n",
    "\n",
    "# Compute z-score for each day’s NB_VALD\n",
    "# Handle cases where std is zero by replacing with a small number\n",
    "ot_grouped[\"std_val\"] = ot_grouped[\"std_val\"].replace(0, np.nan)\n",
    "ot_grouped[\"z_score\"] = (ot_grouped[\"NB_VALD\"] - ot_grouped[\"mean_val\"]) / ot_grouped[\n",
    "    \"std_val\"\n",
    "]\n",
    "\n",
    "# Define an anomaly threshold\n",
    "# For example, we consider a strike if it's 3 standard deviations below the mean\n",
    "threshold = -3\n",
    "\n",
    "# Flag potential strike days\n",
    "ot_grouped[\"potential_strike\"] = ot_grouped[\"z_score\"] < threshold\n",
    "\n",
    "# Optionally, select only those rows that are flagged\n",
    "possible_strike_days = ot_grouped[ot_grouped[\"potential_strike\"]]\n",
    "\n",
    "print(\"Potential strike days:\")\n",
    "possible_strike_days[[\"JOUR\", \"NB_VALD\", \"z_score\", \"potential_strike\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_strike_days.groupby(\"JOUR\")[\"potential_strike\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using t-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At station level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by day and station, sum NB_VALD\n",
    "ot_grouped = (\n",
    "    overground_transport.groupby([\"JOUR\", \"CODE_STIF_LIGNE\"])[\"NB_VALD\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "ot_grouped = ot_grouped[\n",
    "    (ot_grouped[\"JOUR\"] >= \"2020-09-01\") & (ot_grouped[\"JOUR\"] <= \"2021-10-18\")\n",
    "]\n",
    "\n",
    "# Ensure JOUR is a datetime type\n",
    "ot_grouped[\"JOUR\"] = pd.to_datetime(ot_grouped[\"JOUR\"])\n",
    "\n",
    "# Extract the day of week (0=Monday, 6=Sunday)\n",
    "ot_grouped[\"day_of_week\"] = ot_grouped[\"JOUR\"].dt.dayofweek\n",
    "\n",
    "# Compute baseline stats (mean, std, and count) per day_of_week and station\n",
    "baseline_stats = (\n",
    "    ot_grouped.groupby([\"CODE_STIF_LIGNE\", \"day_of_week\"])[\"NB_VALD\"]\n",
    "    .agg([\"mean\", \"std\", \"count\"])\n",
    "    .reset_index()\n",
    ")\n",
    "baseline_stats.rename(\n",
    "    columns={\"mean\": \"mean_val\", \"std\": \"std_val\", \"count\": \"n\"}, inplace=True\n",
    ")\n",
    "\n",
    "# Merge the baseline stats back to the original DataFrame\n",
    "ot_grouped = ot_grouped.merge(\n",
    "    baseline_stats, on=[\"CODE_STIF_LIGNE\", \"day_of_week\"], how=\"left\"\n",
    ")\n",
    "\n",
    "# Compute studentized residuals for each day’s NB_VALD\n",
    "ot_grouped[\"std_val\"] = ot_grouped[\"std_val\"].replace(0, np.nan)\n",
    "ot_grouped[\"studentized_residual\"] = (\n",
    "    (ot_grouped[\"NB_VALD\"] - ot_grouped[\"mean_val\"]) / ot_grouped[\"std_val\"]\n",
    ") * np.sqrt((ot_grouped[\"n\"] + 1) / ot_grouped[\"n\"])\n",
    "\n",
    "# Define an anomaly threshold using the t-distribution\n",
    "threshold = t.ppf(\n",
    "    0.025, df=ot_grouped[\"n\"] - 1\n",
    ")  # 2.5% significance level for two-tailed test\n",
    "\n",
    "# Flag potential strike days\n",
    "ot_grouped[\"potential_strike\"] = ot_grouped[\"studentized_residual\"] < threshold\n",
    "\n",
    "# Optionally, select only those rows that are flagged\n",
    "possible_strike_days = ot_grouped[ot_grouped[\"potential_strike\"]]\n",
    "\n",
    "print(\"Potential strike days:\")\n",
    "possible_strike_days[\n",
    "    [\"JOUR\", \"CODE_STIF_LIGNE\", \"NB_VALD\", \"studentized_residual\", \"potential_strike\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_strike_days.groupby(\"JOUR\")[\"potential_strike\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At daily level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by day and station, sum NB_VALD\n",
    "ot_grouped = overground_transport.groupby([\"JOUR\"])[\"NB_VALD\"].sum().reset_index()\n",
    "ot_grouped = ot_grouped[\n",
    "    (ot_grouped[\"JOUR\"] >= \"2020-09-01\") & (ot_grouped[\"JOUR\"] <= \"2021-10-18\")\n",
    "]\n",
    "\n",
    "# Ensure JOUR is a datetime type\n",
    "ot_grouped[\"JOUR\"] = pd.to_datetime(ot_grouped[\"JOUR\"])\n",
    "\n",
    "# Extract the day of week (0=Monday, 6=Sunday)\n",
    "ot_grouped[\"day_of_week\"] = ot_grouped[\"JOUR\"].dt.dayofweek\n",
    "\n",
    "# Compute baseline stats (mean, std, and count) per day_of_week and station\n",
    "baseline_stats = (\n",
    "    ot_grouped.groupby([\"day_of_week\"])[\"NB_VALD\"]\n",
    "    .agg([\"mean\", \"std\", \"count\"])\n",
    "    .reset_index()\n",
    ")\n",
    "baseline_stats.rename(\n",
    "    columns={\"mean\": \"mean_val\", \"std\": \"std_val\", \"count\": \"n\"}, inplace=True\n",
    ")\n",
    "\n",
    "# Merge the baseline stats back to the original DataFrame\n",
    "ot_grouped = ot_grouped.merge(baseline_stats, on=[\"day_of_week\"], how=\"left\")\n",
    "\n",
    "# Compute studentized residuals for each day’s NB_VALD\n",
    "ot_grouped[\"std_val\"] = ot_grouped[\"std_val\"].replace(0, np.nan)\n",
    "ot_grouped[\"studentized_residual\"] = (\n",
    "    (ot_grouped[\"NB_VALD\"] - ot_grouped[\"mean_val\"]) / ot_grouped[\"std_val\"]\n",
    ") * np.sqrt((ot_grouped[\"n\"] + 1) / ot_grouped[\"n\"])\n",
    "\n",
    "# Define an anomaly threshold using the t-distribution\n",
    "threshold = t.ppf(\n",
    "    0.025, df=ot_grouped[\"n\"] - 1\n",
    ")  # 2.5% significance level for two-tailed test\n",
    "\n",
    "# Flag potential strike days\n",
    "ot_grouped[\"potential_strike\"] = ot_grouped[\"studentized_residual\"] < threshold\n",
    "\n",
    "# Optionally, select only those rows that are flagged\n",
    "possible_strike_days = ot_grouped[ot_grouped[\"potential_strike\"]]\n",
    "\n",
    "print(\"Potential strike days:\")\n",
    "possible_strike_days[[\"JOUR\", \"NB_VALD\", \"studentized_residual\", \"potential_strike\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_strike_days.groupby(\"JOUR\")[\"potential_strike\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging underground and overground transport strike data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
