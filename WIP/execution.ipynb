{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import datetime\n",
    "import io\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "# Third-Party Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from scipy.stats import mode\n",
    "import xgboost as xgb\n",
    "\n",
    "from skrub import TableVectorizer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    HistGradientBoostingRegressor,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# from sklearn.preprocessing import (\n",
    "#     FunctionTransformer,\n",
    "#     OneHotEncoder,\n",
    "#     OrdinalEncoder,\n",
    "#     StandardScaler,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import provided data\n",
    "train = pd.read_parquet(\"/Users/pierrehaas/bike_counters/data/train.parquet\")\n",
    "test = pd.read_parquet(\"/Users/pierrehaas/bike_counters/data/final_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additionally sourced data\n",
    "\n",
    "# https://meteo.data.gouv.fr/datasets/donnees-climatologiques-de-base-horaires/\n",
    "weather = pd.read_csv(\n",
    "    \"/Users/pierrehaas/bike_counters/external_data/weather/H_75_previous-2020-2022.csv.gz\",\n",
    "    parse_dates=[\"AAAAMMJJHH\"],\n",
    "    date_format=\"%Y%m%d%H\",\n",
    "    compression=\"gzip\",\n",
    "    sep=\";\",\n",
    ").rename(columns={\"AAAAMMJJHH\": \"date\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public transport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/information/\n",
    "# URLs of the zip files\n",
    "urls = [\n",
    "    \"https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/files/e6bcf4c994951fc086e31db6819a3448/download/\",\n",
    "    \"https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/files/e35b9ec0a183a8f2c7a8537dd43b124c/download/\",\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# File matching pattern\n",
    "pattern = r\"data-rf-202\\d/202\\d_S\\d+_NB_FER\\.txt\"\n",
    "\n",
    "# Process each ZIP file\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            # Get a list of all files in the archive and filter matching files\n",
    "            matching_files = [f for f in z.namelist() if re.match(pattern, f)]\n",
    "\n",
    "            # Read and concatenate the matching files\n",
    "            for file in matching_files:\n",
    "                with z.open(file) as f:\n",
    "                    # Assuming the files are tab-separated and have a \"JOUR\" column\n",
    "                    df = pd.read_csv(f, sep=\"\\t\", parse_dates=[\"JOUR\"], dayfirst=True)\n",
    "                    dfs.append(df)\n",
    "\n",
    "# Combine all DataFrames\n",
    "underground_transport = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "\n",
    "# # https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-surface/information/\n",
    "# # URLs of the zip files\n",
    "urls = [\n",
    "    \"https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-surface/files/41adcbd4216382c232ced4ccbf60187e/download/\",\n",
    "    \"https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-surface/files/68cac32e8717f476905a60006a4dca26/download/\",\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# File matching pattern\n",
    "pattern = r\"data-rs-202\\d/202\\d_T\\d+_NB_SURFACE\\.txt\"\n",
    "\n",
    "# Process each ZIP file\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            # Get a list of all files in the archive and filter matching files\n",
    "            matching_files = [f for f in z.namelist() if re.match(pattern, f)]\n",
    "\n",
    "            # Read and concatenate the matching files\n",
    "            for file in matching_files:\n",
    "                with z.open(file) as f:\n",
    "                    # Assuming the files are tab-separated and have a \"JOUR\" column\n",
    "                    df = pd.read_csv(\n",
    "                        f,\n",
    "                        sep=\"\\t\",\n",
    "                        parse_dates=[\"JOUR\"],\n",
    "                        dayfirst=True,\n",
    "                        encoding=\"latin1\",\n",
    "                    )\n",
    "                    dfs.append(df)\n",
    "\n",
    "# Combine all DataFrames\n",
    "overground_transport = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Car traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs of the zip files\n",
    "urls = [\n",
    "    \"https://parisdata.opendatasoft.com/api/datasets/1.0/comptages-routiers-permanents-historique/attachments/opendata_txt_2020_zip/\",\n",
    "    # \"https://parisdata.opendatasoft.com/api/datasets/1.0/comptages-routiers-permanents-historique/attachments/opendata_txt_2021_zip/\", # This file's compression format is broken, thus I provide a download link below\n",
    "    \"https://www.dropbox.com/scl/fi/sfqzlzpyxcf4yied3yucc/comptage-routier-2021.zip?rlkey=6k6hr3kywl8tvm4ax1qv2nv88&st=ktehiium&dl=1\",\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Process each ZIP file\n",
    "for i, url in enumerate(urls):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            # Process every file in the archive\n",
    "            for file in z.namelist():\n",
    "                # Skip directories and __MACOSX files\n",
    "                if file.endswith(\"/\") or \"__MACOSX\" in file:\n",
    "                    continue\n",
    "                # For the second URL, ensure files are within \"comptage-routier-2021\" directory\n",
    "                if i == 1 and not file.startswith(\"comptage-routier-2021/\"):\n",
    "                    continue\n",
    "                with z.open(file) as f:\n",
    "                    # Assuming the files are semicolon-separated and have a \"t_1h\" column\n",
    "                    df = pd.read_csv(f, sep=\";\", parse_dates=[\"t_1h\"])\n",
    "                    dfs.append(df)\n",
    "\n",
    "# Combine all DataFrames\n",
    "cars_count = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_encoder(X, col=\"date\"):\n",
    "    X = X.copy()  # modify a copy of X\n",
    "\n",
    "    # Encode the date information from the DateOfDeparture columns\n",
    "    X[\"year\"] = X[col].dt.year\n",
    "    X[\"quarter\"] = X[col].dt.quarter\n",
    "    X[\"month\"] = X[col].dt.month\n",
    "    X[\"day\"] = X[col].dt.day\n",
    "    X[\"weekday\"] = X[col].dt.weekday + 1\n",
    "    X[\"hour\"] = X[col].dt.hour\n",
    "    X[\"hour_scaled\"] = np.cos(2 * np.pi * X[\"hour\"] / 24)\n",
    "\n",
    "    # Binary variable indicating weekend or not (1=weekend, 0=weekday)\n",
    "    X[\"is_weekend\"] = (X[\"weekday\"] > 5).astype(int)\n",
    "\n",
    "    # Binary variable indicating bank holiday or not (1=holiday, 0=not holiday)\n",
    "    import holidays\n",
    "\n",
    "    fr_bank_holidays = holidays.FR()  # Get list of FR holidays\n",
    "    X[\"is_bank_holiday\"] = X[col].apply(lambda x: 1 if x in fr_bank_holidays else 0)\n",
    "\n",
    "    X = X.copy()  # modify a copy of X\n",
    "\n",
    "    # Binary variable indicating school holiday or not (1=holiday, 0=not holiday)\n",
    "    # https://www.data.gouv.fr/fr/datasets/vacances-scolaires-par-zones/\n",
    "    fr_school_holidays = pd.read_csv(\n",
    "        \"/Users/pierrehaas/bike_counters/external_data/vacances_scolaires_france.csv\"\n",
    "    )[[\"date\", \"vacances_zone_c\"]]\n",
    "\n",
    "    # Ensure both DataFrames have a consistent datetime format\n",
    "    X[\"date_normalized\"] = pd.to_datetime(X[col]).dt.normalize()\n",
    "    fr_school_holidays[\"date\"] = pd.to_datetime(\n",
    "        fr_school_holidays[\"date\"]\n",
    "    ).dt.normalize()\n",
    "\n",
    "    # Create a dictionary from the holidays dataset for faster lookup\n",
    "    holiday_mapping = dict(\n",
    "        zip(fr_school_holidays[\"date\"], fr_school_holidays[\"vacances_zone_c\"])\n",
    "    )\n",
    "\n",
    "    # Map the normalized date to the holiday column\n",
    "    X[\"is_school_holiday\"] = (\n",
    "        X[\"date_normalized\"].map(holiday_mapping).fillna(0).astype(int)\n",
    "    )\n",
    "\n",
    "    # Drop the normalized date column if not needed\n",
    "    X.drop(columns=[\"date_normalized\"], inplace=True)\n",
    "\n",
    "    # Finally, return the updated DataFrame\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering on train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_processing(X_train, X_test):\n",
    "\n",
    "    def dist_station(X):\n",
    "\n",
    "        lat_long_station = (\n",
    "            X[[\"latitude\", \"longitude\"]]\n",
    "            - mode(\n",
    "                X[X[\"site_name\"] == \"Totem 73 boulevard de Sébastopol\"][\n",
    "                    [\"latitude\", \"longitude\"]\n",
    "                ]\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "        # We calculate the distance the previously identified site to all others\n",
    "        # We call the result the distance to the center since the station is close to the center of Paris\n",
    "        dist_station = np.linalg.norm(\n",
    "            lat_long_station,\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        return dist_station\n",
    "\n",
    "    X_train[\"dist_to_station\"] = dist_station(X_train)\n",
    "    X_test[\"dist_to_station\"] = dist_station(X_test)\n",
    "\n",
    "    # Group by counter_name and calculate the mean log_bike_count\n",
    "    grouped_train = (\n",
    "        X_train.groupby(\"counter_name\", observed=True)[\"log_bike_count\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Reshape the data for clustering\n",
    "    Y = grouped_train[[\"log_bike_count\"]]\n",
    "\n",
    "    # Apply K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=5)\n",
    "    grouped_train[\"cluster\"] = kmeans.fit_predict(Y)\n",
    "\n",
    "    # Sort clusters by mean log_bike_count and reassign cluster labels\n",
    "    sorted_clusters = (\n",
    "        grouped_train.groupby(\"cluster\")[\"log_bike_count\"].mean().sort_values().index\n",
    "    )\n",
    "    cluster_mapping = {\n",
    "        old_label: new_label for new_label, old_label in enumerate(sorted_clusters)\n",
    "    }\n",
    "    grouped_train[\"cluster\"] = grouped_train[\"cluster\"].map(cluster_mapping)\n",
    "\n",
    "    # Merge the cluster labels back to the original DataFrame\n",
    "    X_train = X_train.merge(\n",
    "        grouped_train[[\"counter_name\", \"cluster\"]], on=\"counter_name\", how=\"left\"\n",
    "    )\n",
    "    X_test = X_test.merge(\n",
    "        grouped_train[[\"counter_name\", \"cluster\"]], on=\"counter_name\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering on weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_processing(X, train_min, train_max, test_min, test_max):\n",
    "\n",
    "    X_reduced = (\n",
    "        X.drop(columns=[\"NUM_POSTE\", \"NOM_USUEL\", \"LAT\", \"LON\", \"QDXI3S\"])\n",
    "        .groupby(\"date\")\n",
    "        .mean()\n",
    "        .dropna(axis=1, how=\"all\")\n",
    "        .interpolate(method=\"linear\")\n",
    "    )\n",
    "\n",
    "    X_reduced[\"is_rain\"] = (X_reduced[\"RR1\"] > 0).astype(int)\n",
    "\n",
    "    X_reduced[\"q_rain_lag_1h\"] = X_reduced[\"RR1\"].shift(1)\n",
    "    X_reduced[\"t_rain_lag_1h\"] = X_reduced[\"DRR1\"].shift(1)\n",
    "    X_reduced[\"is_rain_lag_1h\"] = X_reduced[\"is_rain\"].shift(1)\n",
    "\n",
    "    X_reduced[\"q_rain_next_1h\"] = X_reduced[\"RR1\"].shift(-1)\n",
    "    X_reduced[\"t_rain_next_1h\"] = X_reduced[\"DRR1\"].shift(-1)\n",
    "    X_reduced[\"is_rain_next_1h\"] = X_reduced[\"is_rain\"].shift(-1)\n",
    "\n",
    "    X_reduced[\"temp_lag_1h\"] = X_reduced[\"T\"].shift(1)\n",
    "    X_reduced[\"temp_next_1h\"] = X_reduced[\"T\"].shift(-1)\n",
    "\n",
    "    X_reduced[\"max_temp\"] = X_reduced.groupby(X_reduced.index.date)[\"T\"].transform(\n",
    "        \"max\"\n",
    "    )\n",
    "    X_reduced[\"will_rain\"] = (\n",
    "        X_reduced.groupby(X_reduced.index.date)[\"RR1\"]\n",
    "        .transform(lambda x: (x > 0).any())\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    weather_features = [\n",
    "        \"RR1\",\n",
    "        \"DRR1\",\n",
    "        \"T\",\n",
    "        \"TNSOL\",\n",
    "        \"TCHAUSSEE\",\n",
    "        \"U\",\n",
    "        \"GLO\",\n",
    "        # \"is_rain\",\n",
    "        \"q_rain_lag_1h\",\n",
    "        \"t_rain_lag_1h\",\n",
    "        # \"is_rain_lag_1h\",\n",
    "        \"q_rain_next_1h\",\n",
    "        \"t_rain_next_1h\",\n",
    "        # \"is_rain_next_1h\",\n",
    "        \"temp_lag_1h\",\n",
    "        \"temp_next_1h\",\n",
    "        \"max_temp\",\n",
    "        # \"will_rain\",\n",
    "    ]\n",
    "\n",
    "    X_reduced_train = X_reduced[\n",
    "        (X_reduced.index >= train_min) & (X_reduced.index <= train_max)\n",
    "    ]\n",
    "\n",
    "    X_reduced_test = X_reduced[\n",
    "        (X_reduced.index >= test_min) & (X_reduced.index <= test_max)\n",
    "    ]\n",
    "\n",
    "    n = 5\n",
    "    pca = PCA(n_components=n)\n",
    "\n",
    "    pca.fit(X_reduced_train[weather_features])\n",
    "\n",
    "    X_pca_train = pca.transform(X_reduced_train[weather_features])\n",
    "    X_pca_test = pca.transform(X_reduced_test[weather_features])\n",
    "\n",
    "    X_pca_train = pd.DataFrame(\n",
    "        X_pca_train,\n",
    "        index=X_reduced_train[weather_features].index,\n",
    "        columns=[\"weather_\" + str(i) for i in range(1, n + 1)],\n",
    "    ).reset_index()\n",
    "\n",
    "    X_pca_test = pd.DataFrame(\n",
    "        X_pca_test,\n",
    "        index=X_reduced_test[weather_features].index,\n",
    "        columns=[\"weather_\" + str(i) for i in range(1, n + 1)],\n",
    "    ).reset_index()\n",
    "\n",
    "    X_pca = pd.concat([X_pca_train, X_pca_test], ignore_index=True)\n",
    "\n",
    "    X_pca = X_pca.merge(\n",
    "        X_reduced[(X_reduced.index >= train_min) & (X_reduced.index <= test_max)][\n",
    "            [\"is_rain\", \"is_rain_lag_1h\", \"is_rain_next_1h\", \"will_rain\"]\n",
    "        ],\n",
    "        on=\"date\",\n",
    "    )\n",
    "\n",
    "    return X_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing of public transport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transport_processing(X1, X2, train_min=\"2020-09-01\", test_max=\"2021-10-18\"):\n",
    "\n",
    "    daily_X1 = X1.groupby(\"JOUR\")[\"NB_VALD\"].sum()\n",
    "    daily_X2 = X2.groupby(\"JOUR\")[\"NB_VALD\"].sum()\n",
    "\n",
    "    X = (daily_X1 + daily_X2).reset_index()\n",
    "\n",
    "    X_reduced = date_encoder(X, col=\"JOUR\")[\n",
    "        (X[\"JOUR\"] >= train_min) & (X[\"JOUR\"] <= test_max)\n",
    "    ]\n",
    "\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing of car traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def car_traffic_processing(X, train_min, test_max):\n",
    "    X_hourly = (\n",
    "        X[\n",
    "            (X[\"t_1h\"] >= train_min - datetime.timedelta(hours=1))\n",
    "            & (X[\"t_1h\"] <= test_max + datetime.timedelta(hours=1))\n",
    "        ]\n",
    "        .groupby(\"t_1h\")[\"q\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Group by day and calculate the cumulative sum of 'q' for each day\n",
    "    X_hourly[\"daily_cumsum\"] = X_hourly.groupby(X_hourly[\"t_1h\"].dt.to_period(\"d\"))[\n",
    "        \"q\"\n",
    "    ].cumsum()\n",
    "\n",
    "    return X_hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging of all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_engineered(\n",
    "    df_train, df_test, weather, underground_transport, overground_transport, cars_count\n",
    "):\n",
    "    \"\"\"\n",
    "    Merging the data\n",
    "    \"\"\"\n",
    "\n",
    "    def merging_data(data, weather, public_transport, car_traffic):\n",
    "        data = data.merge(weather, on=\"date\", how=\"left\")\n",
    "        data = data.merge(\n",
    "            public_transport[[\"year\", \"month\", \"day\", \"NB_VALD\"]],\n",
    "            on=[\"year\", \"month\", \"day\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        data = data.merge(car_traffic, left_on=\"date\", right_on=\"t_1h\", how=\"left\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    train_min, train_max = df_train[\"date\"].min(), df_train[\"date\"].max()\n",
    "    test_min, test_max = df_test[\"date\"].min(), df_test[\"date\"].max()\n",
    "\n",
    "    # Encoding the date\n",
    "    df_train, df_test = date_encoder(df_train), date_encoder(df_test)\n",
    "\n",
    "    # Processing the data\n",
    "    df_train, df_test = train_test_processing(df_train, df_test)\n",
    "\n",
    "    # Processing weather data\n",
    "    weather_processed = weather_processing(\n",
    "        weather, train_min, train_max, test_min, test_max\n",
    "    )\n",
    "\n",
    "    # Processing transport data\n",
    "    transport_processed = transport_processing(\n",
    "        underground_transport, overground_transport\n",
    "    )\n",
    "\n",
    "    # Processing car traffic data\n",
    "    car_traffic_processed = car_traffic_processing(cars_count, train_min, test_max)\n",
    "\n",
    "    # Merging the data\n",
    "    df_train = merging_data(\n",
    "        df_train, weather_processed, transport_processed, car_traffic_processed\n",
    "    )\n",
    "\n",
    "    df_test = merging_data(\n",
    "        df_test, weather_processed, transport_processed, car_traffic_processed\n",
    "    )\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = data_engineered(\n",
    "    train, test, weather, underground_transport, overground_transport, cars_count\n",
    ")\n",
    "\n",
    "# X_train = X_train[X_train[\"date\"] >= \"2021-05-01\"].dropna()\n",
    "X_train = X_train.dropna()\n",
    "y_train = X_train[\"log_bike_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TableVectorizer()\n",
    "# model = xgb.XGBRegressor()\n",
    "# model = RandomForestRegressor()\n",
    "model = ExtraTreesRegressor()\n",
    "pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"model\", model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# # When using a scorer in scikit-learn it always needs to be better when smaller, hence the minus sign.\n",
    "# scores = cross_val_score(\n",
    "#     pipe, X_train, y_train, cv=cv, scoring=\"neg_root_mean_squared_error\"\n",
    "# )\n",
    "# print(\"RMSE: \", scores)\n",
    "# print(f\"RMSE (all folds): {-scores.mean():.3} ± {(-scores).std():.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# # When using a scorer in scikit-learn it always needs to be better when smaller, hence the minus sign.\n",
    "# scores = cross_val_score(\n",
    "#     pipe, X_train, y_train, cv=cv, scoring=\"neg_root_mean_squared_error\"\n",
    "# )\n",
    "# print(\"RMSE: \", scores)\n",
    "# print(f\"RMSE (all folds): {-scores.mean():.3} ± {(-scores).std():.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train.drop(columns=[\"bike_count\", \"log_bike_count\"]), y_train)\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(y_train, bins=100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(y_pred, bins=100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_pred, columns=[\"log_bike_count\"]).reset_index().rename(\n",
    "    columns={\"index\": \"Id\"}\n",
    ").to_csv(\"/Users/pierrehaas/bike_counters/predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle competitions submit -c msdb-2024 -f submission.csv -m \"Message\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
