{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import datetime\n",
    "import io\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "# Third-Party Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from scipy.stats import mode\n",
    "import xgboost as xgb\n",
    "\n",
    "from skrub import TableVectorizer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    StandardScaler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import provided data\n",
    "train = pd.read_parquet(\"/Users/pierrehaas/bike_counters/data/train.parquet\")\n",
    "test = pd.read_parquet(\"/Users/pierrehaas/bike_counters/data/final_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additionally sourced data\n",
    "\n",
    "# https://meteo.data.gouv.fr/datasets/donnees-climatologiques-de-base-horaires/\n",
    "weather = pd.read_csv(\n",
    "    \"/Users/pierrehaas/bike_counters/external_data/weather/H_75_previous-2020-2022.csv.gz\",\n",
    "    parse_dates=[\"AAAAMMJJHH\"],\n",
    "    date_format=\"%Y%m%d%H\",\n",
    "    compression=\"gzip\",\n",
    "    sep=\";\",\n",
    ").rename(columns={\"AAAAMMJJHH\": \"date\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public transport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/information/\n",
    "# URLs of the zip files\n",
    "urls = [\n",
    "    \"https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/files/e6bcf4c994951fc086e31db6819a3448/download/\",\n",
    "    \"https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/files/e35b9ec0a183a8f2c7a8537dd43b124c/download/\",\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# File matching pattern\n",
    "pattern = r\"data-rf-202\\d/202\\d_S\\d+_NB_FER\\.txt\"\n",
    "\n",
    "# Process each ZIP file\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            # Get a list of all files in the archive and filter matching files\n",
    "            matching_files = [f for f in z.namelist() if re.match(pattern, f)]\n",
    "\n",
    "            # Read and concatenate the matching files\n",
    "            for file in matching_files:\n",
    "                with z.open(file) as f:\n",
    "                    # Assuming the files are tab-separated and have a \"JOUR\" column\n",
    "                    df = pd.read_csv(f, sep=\"\\t\", parse_dates=[\"JOUR\"], dayfirst=True)\n",
    "                    dfs.append(df)\n",
    "\n",
    "# Combine all DataFrames\n",
    "underground_transport = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "\n",
    "# # https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-surface/information/\n",
    "# # URLs of the zip files\n",
    "urls = [\n",
    "    \"https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-surface/files/41adcbd4216382c232ced4ccbf60187e/download/\",\n",
    "    \"https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-surface/files/68cac32e8717f476905a60006a4dca26/download/\",\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# File matching pattern\n",
    "pattern = r\"data-rs-202\\d/202\\d_T\\d+_NB_SURFACE\\.txt\"\n",
    "\n",
    "# Process each ZIP file\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            # Get a list of all files in the archive and filter matching files\n",
    "            matching_files = [f for f in z.namelist() if re.match(pattern, f)]\n",
    "\n",
    "            # Read and concatenate the matching files\n",
    "            for file in matching_files:\n",
    "                with z.open(file) as f:\n",
    "                    # Assuming the files are tab-separated and have a \"JOUR\" column\n",
    "                    df = pd.read_csv(\n",
    "                        f,\n",
    "                        sep=\"\\t\",\n",
    "                        parse_dates=[\"JOUR\"],\n",
    "                        dayfirst=True,\n",
    "                        encoding=\"latin1\",\n",
    "                    )\n",
    "                    dfs.append(df)\n",
    "\n",
    "# Combine all DataFrames\n",
    "overground_transport = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Car traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iu_ac</th>\n",
       "      <th>libelle</th>\n",
       "      <th>iu_nd_amont</th>\n",
       "      <th>libelle_nd_amont</th>\n",
       "      <th>iu_nd_aval</th>\n",
       "      <th>libelle_nd_aval</th>\n",
       "      <th>t_1h</th>\n",
       "      <th>q</th>\n",
       "      <th>k</th>\n",
       "      <th>etat_trafic</th>\n",
       "      <th>etat_barre</th>\n",
       "      <th>dessin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>799</td>\n",
       "      <td>Bd_Kellermann</td>\n",
       "      <td>460</td>\n",
       "      <td>Bd_Kellermann-Moulin_Pointe</td>\n",
       "      <td>459</td>\n",
       "      <td>Bd_Kellermann-Damesme</td>\n",
       "      <td>2020-01-01 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>\"&lt;PLINE COURBE=\"\"1\"\"&gt;&lt;PT X=\"\"601352\"\" Y=\"\"1245...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>799</td>\n",
       "      <td>Bd_Kellermann</td>\n",
       "      <td>460</td>\n",
       "      <td>Bd_Kellermann-Moulin_Pointe</td>\n",
       "      <td>459</td>\n",
       "      <td>Bd_Kellermann-Damesme</td>\n",
       "      <td>2020-01-01 02:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>\"&lt;PLINE COURBE=\"\"1\"\"&gt;&lt;PT X=\"\"601352\"\" Y=\"\"1245...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>799</td>\n",
       "      <td>Bd_Kellermann</td>\n",
       "      <td>460</td>\n",
       "      <td>Bd_Kellermann-Moulin_Pointe</td>\n",
       "      <td>459</td>\n",
       "      <td>Bd_Kellermann-Damesme</td>\n",
       "      <td>2020-01-01 03:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>\"&lt;PLINE COURBE=\"\"1\"\"&gt;&lt;PT X=\"\"601352\"\" Y=\"\"1245...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>799</td>\n",
       "      <td>Bd_Kellermann</td>\n",
       "      <td>460</td>\n",
       "      <td>Bd_Kellermann-Moulin_Pointe</td>\n",
       "      <td>459</td>\n",
       "      <td>Bd_Kellermann-Damesme</td>\n",
       "      <td>2020-01-01 04:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>\"&lt;PLINE COURBE=\"\"1\"\"&gt;&lt;PT X=\"\"601352\"\" Y=\"\"1245...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>799</td>\n",
       "      <td>Bd_Kellermann</td>\n",
       "      <td>460</td>\n",
       "      <td>Bd_Kellermann-Moulin_Pointe</td>\n",
       "      <td>459</td>\n",
       "      <td>Bd_Kellermann-Damesme</td>\n",
       "      <td>2020-01-01 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>\"&lt;PLINE COURBE=\"\"1\"\"&gt;&lt;PT X=\"\"601352\"\" Y=\"\"1245...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iu_ac        libelle  iu_nd_amont             libelle_nd_amont  iu_nd_aval  \\\n",
       "0    799  Bd_Kellermann          460  Bd_Kellermann-Moulin_Pointe         459   \n",
       "1    799  Bd_Kellermann          460  Bd_Kellermann-Moulin_Pointe         459   \n",
       "2    799  Bd_Kellermann          460  Bd_Kellermann-Moulin_Pointe         459   \n",
       "3    799  Bd_Kellermann          460  Bd_Kellermann-Moulin_Pointe         459   \n",
       "4    799  Bd_Kellermann          460  Bd_Kellermann-Moulin_Pointe         459   \n",
       "\n",
       "         libelle_nd_aval                t_1h   q   k  etat_trafic  etat_barre  \\\n",
       "0  Bd_Kellermann-Damesme 2020-01-01 01:00:00 NaN NaN            0           3   \n",
       "1  Bd_Kellermann-Damesme 2020-01-01 02:00:00 NaN NaN            0           3   \n",
       "2  Bd_Kellermann-Damesme 2020-01-01 03:00:00 NaN NaN            0           3   \n",
       "3  Bd_Kellermann-Damesme 2020-01-01 04:00:00 NaN NaN            0           3   \n",
       "4  Bd_Kellermann-Damesme 2020-01-01 05:00:00 NaN NaN            0           3   \n",
       "\n",
       "                                              dessin  \n",
       "0  \"<PLINE COURBE=\"\"1\"\"><PT X=\"\"601352\"\" Y=\"\"1245...  \n",
       "1  \"<PLINE COURBE=\"\"1\"\"><PT X=\"\"601352\"\" Y=\"\"1245...  \n",
       "2  \"<PLINE COURBE=\"\"1\"\"><PT X=\"\"601352\"\" Y=\"\"1245...  \n",
       "3  \"<PLINE COURBE=\"\"1\"\"><PT X=\"\"601352\"\" Y=\"\"1245...  \n",
       "4  \"<PLINE COURBE=\"\"1\"\"><PT X=\"\"601352\"\" Y=\"\"1245...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URLs of the zip files\n",
    "urls = [\n",
    "    \"https://parisdata.opendatasoft.com/api/datasets/1.0/comptages-routiers-permanents-historique/attachments/opendata_txt_2020_zip/\",\n",
    "    # \"https://parisdata.opendatasoft.com/api/datasets/1.0/comptages-routiers-permanents-historique/attachments/opendata_txt_2021_zip/\", # This file's compression format is broken, thus I provide a download link below\n",
    "    \"https://www.dropbox.com/scl/fi/sfqzlzpyxcf4yied3yucc/comptage-routier-2021.zip?rlkey=6k6hr3kywl8tvm4ax1qv2nv88&st=ktehiium&dl=1\",\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Process each ZIP file\n",
    "for i, url in enumerate(urls):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            # Process every file in the archive\n",
    "            for file in z.namelist():\n",
    "                # Skip directories and __MACOSX files\n",
    "                if file.endswith(\"/\") or \"__MACOSX\" in file:\n",
    "                    continue\n",
    "                # For the second URL, ensure files are within \"comptage-routier-2021\" directory\n",
    "                if i == 1 and not file.startswith(\"comptage-routier-2021/\"):\n",
    "                    continue\n",
    "                with z.open(file) as f:\n",
    "                    # Assuming the files are semicolon-separated and have a \"t_1h\" column\n",
    "                    df = pd.read_csv(f, sep=\";\", parse_dates=[\"t_1h\"])\n",
    "                    dfs.append(df)\n",
    "\n",
    "# Combine all DataFrames\n",
    "cars_count = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "cars_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_min, test_max = train[\"date\"].min(), test[\"date\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_encoder(X, col=\"date\"):\n",
    "    X = X.copy()  # modify a copy of X\n",
    "\n",
    "    # Encode the date information from the DateOfDeparture columns\n",
    "    X[\"year\"] = X[col].dt.year\n",
    "    X[\"quarter\"] = X[col].dt.quarter\n",
    "    X[\"month\"] = X[col].dt.month\n",
    "    X[\"day\"] = X[col].dt.day\n",
    "    X[\"weekday\"] = X[col].dt.weekday + 1\n",
    "    X[\"hour\"] = X[col].dt.hour\n",
    "\n",
    "    # Binary variable indicating weekend or not (1=weekend, 0=weekday)\n",
    "    X[\"is_weekend\"] = (X[\"weekday\"] > 5).astype(int)\n",
    "\n",
    "    # Binary variable indicating bank holiday or not (1=holiday, 0=not holiday)\n",
    "    import holidays\n",
    "\n",
    "    fr_bank_holidays = holidays.FR()  # Get list of FR holidays\n",
    "    X[\"is_bank_holiday\"] = X[col].apply(lambda x: 1 if x in fr_bank_holidays else 0)\n",
    "\n",
    "    X = X.copy()  # modify a copy of X\n",
    "\n",
    "    # Binary variable indicating school holiday or not (1=holiday, 0=not holiday)\n",
    "    # https://www.data.gouv.fr/fr/datasets/vacances-scolaires-par-zones/\n",
    "    fr_school_holidays = pd.read_csv(\n",
    "        \"/Users/pierrehaas/bike_counters/external_data/vacances_scolaires_france.csv\"\n",
    "    )[[\"date\", \"vacances_zone_c\"]]\n",
    "\n",
    "    # Ensure both DataFrames have a consistent datetime format\n",
    "    X[\"date_normalized\"] = pd.to_datetime(X[col]).dt.normalize()\n",
    "    fr_school_holidays[\"date\"] = pd.to_datetime(\n",
    "        fr_school_holidays[\"date\"]\n",
    "    ).dt.normalize()\n",
    "\n",
    "    # Create a dictionary from the holidays dataset for faster lookup\n",
    "    holiday_mapping = dict(\n",
    "        zip(fr_school_holidays[\"date\"], fr_school_holidays[\"vacances_zone_c\"])\n",
    "    )\n",
    "\n",
    "    # Map the normalized date to the holiday column\n",
    "    X[\"is_school_holiday\"] = (\n",
    "        X[\"date_normalized\"].map(holiday_mapping).fillna(0).astype(int)\n",
    "    )\n",
    "\n",
    "    # Drop the normalized date column if not needed\n",
    "    X.drop(columns=[\"date_normalized\"], inplace=True)\n",
    "\n",
    "    # Finally, return the updated DataFrame\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_processing(X_train, X_test):\n",
    "\n",
    "    def dist_station(X):\n",
    "\n",
    "        lat_long_station = (\n",
    "            X[[\"latitude\", \"longitude\"]]\n",
    "            - mode(\n",
    "                X[X[\"site_name\"] == \"Totem 73 boulevard de Sébastopol\"][\n",
    "                    [\"latitude\", \"longitude\"]\n",
    "                ]\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "        # We calculate the distance the previously identified site to all others\n",
    "        # We call the result the distance to the center since the station is close to the center of Paris\n",
    "        dist_station = np.linalg.norm(\n",
    "            lat_long_station,\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        return dist_station\n",
    "\n",
    "    X_train[\"dist_to_station\"] = dist_station(X_train)\n",
    "    X_test[\"dist_to_station\"] = dist_station(X_test)\n",
    "\n",
    "    # Group by counter_name and calculate the mean log_bike_count\n",
    "    grouped_train = (\n",
    "        X_train.groupby(\"counter_name\", observed=True)[\"log_bike_count\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Reshape the data for clustering\n",
    "    Y = grouped_train[[\"log_bike_count\"]]\n",
    "\n",
    "    # Apply K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=5)\n",
    "    grouped_train[\"cluster\"] = kmeans.fit_predict(Y)\n",
    "\n",
    "    # Sort clusters by mean log_bike_count and reassign cluster labels\n",
    "    sorted_clusters = (\n",
    "        grouped_train.groupby(\"cluster\")[\"log_bike_count\"].mean().sort_values().index\n",
    "    )\n",
    "    cluster_mapping = {\n",
    "        old_label: new_label for new_label, old_label in enumerate(sorted_clusters)\n",
    "    }\n",
    "    grouped_train[\"cluster\"] = grouped_train[\"cluster\"].map(cluster_mapping)\n",
    "\n",
    "    # Merge the cluster labels back to the original DataFrame\n",
    "    X_train = X_train.merge(\n",
    "        grouped_train[[\"counter_name\", \"cluster\"]], on=\"counter_name\", how=\"left\"\n",
    "    )\n",
    "    X_test = X_test.merge(\n",
    "        grouped_train[[\"counter_name\", \"cluster\"]], on=\"counter_name\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_processing(X, train_min, test_max):\n",
    "    # Reduce weather to necessary observations\n",
    "    X = X[\n",
    "        (X[\"date\"] >= train_min - datetime.timedelta(hours=1))\n",
    "        & (X[\"date\"] <= test_max + datetime.timedelta(hours=1))\n",
    "    ]\n",
    "\n",
    "    X_reduced = (\n",
    "        X.drop(columns=[\"NUM_POSTE\", \"NOM_USUEL\", \"LAT\", \"LON\", \"QDXI3S\"])\n",
    "        .groupby(\"date\")\n",
    "        .mean()\n",
    "        .dropna(axis=1, how=\"all\")\n",
    "        .interpolate(method=\"linear\")\n",
    "    )\n",
    "\n",
    "    X_reduced[\"is_rain\"] = (X_reduced[\"RR1\"] > 0).astype(int)\n",
    "\n",
    "    weather_features = [\"RR1\", \"DRR1\", \"T\", \"TNSOL\", \"TCHAUSSEE\", \"U\", \"GLO\"]\n",
    "\n",
    "    n = 1\n",
    "    pca = PCA(n_components=n)\n",
    "\n",
    "    X_pca = pca.fit_transform(X_reduced[weather_features])\n",
    "\n",
    "    X_pca = pd.DataFrame(\n",
    "        X_pca,\n",
    "        index=X_reduced[weather_features].index,\n",
    "        columns=[\"weather_\" + str(i) for i in range(1, n + 1)],\n",
    "    ).reset_index()\n",
    "\n",
    "    X_pca = X_pca.merge(X_reduced[[\"is_rain\"]], left_on=\"date\", right_on=\"date\")\n",
    "\n",
    "    return X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transport_processing(X1, X2, train_min=\"2020-09-01\", test_max=\"2021-10-18\"):\n",
    "\n",
    "    daily_X1 = X1.groupby(\"JOUR\")[\"NB_VALD\"].sum()\n",
    "    daily_X2 = X2.groupby(\"JOUR\")[\"NB_VALD\"].sum()\n",
    "\n",
    "    X = (daily_X1 + daily_X2).reset_index()\n",
    "\n",
    "    X_reduced = date_encoder(X, col=\"JOUR\")[\n",
    "        (X[\"JOUR\"] >= train_min) & (X[\"JOUR\"] <= test_max)\n",
    "    ]\n",
    "\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def car_traffic_processing(X, train_min, test_max):\n",
    "    X_hourly = (\n",
    "        X[\n",
    "            (X[\"t_1h\"] >= train_min - datetime.timedelta(hours=1))\n",
    "            & (X[\"t_1h\"] <= test_max + datetime.timedelta(hours=1))\n",
    "        ]\n",
    "        .groupby(\"t_1h\")[\"q\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Group by day and calculate the cumulative sum of 'q' for each day\n",
    "    X_hourly[\"daily_cumsum\"] = X_hourly.groupby(X_hourly[\"t_1h\"].dt.to_period(\"d\"))[\n",
    "        \"q\"\n",
    "    ].cumsum()\n",
    "\n",
    "    return X_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_engineered(\n",
    "    df_train, df_test, weather, underground_transport, overground_transport, cars_count\n",
    "):\n",
    "    \"\"\"\n",
    "    Merging the data\n",
    "    \"\"\"\n",
    "\n",
    "    def merging_data(data, weather, public_transport, car_traffic):\n",
    "        data = data.merge(weather, on=\"date\", how=\"left\")\n",
    "        data = data.merge(\n",
    "            public_transport[[\"year\", \"month\", \"day\", \"NB_VALD\"]],\n",
    "            on=[\"year\", \"month\", \"day\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        data = data.merge(car_traffic, left_on=\"date\", right_on=\"t_1h\", how=\"left\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    # Encoding the date\n",
    "    df_train, df_test = date_encoder(df_train), date_encoder(df_test)\n",
    "\n",
    "    # Processing the data\n",
    "    df_train, df_test = train_test_processing(df_train, df_test)\n",
    "\n",
    "    # Processing weather data\n",
    "    weather_processed = weather_processing(weather, train_min, test_max)\n",
    "\n",
    "    # Processing transport data\n",
    "    transport_processed = transport_processing(\n",
    "        underground_transport, overground_transport\n",
    "    )\n",
    "\n",
    "    # Processing car traffic data\n",
    "    car_traffic_processed = car_traffic_processing(cars_count, train_min, test_max)\n",
    "\n",
    "    # Merging the data\n",
    "    df_train = merging_data(\n",
    "        df_train, weather_processed, transport_processed, car_traffic_processed\n",
    "    )\n",
    "\n",
    "    df_test = merging_data(\n",
    "        df_test, weather_processed, transport_processed, car_traffic_processed\n",
    "    )\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = data_engineered(\n",
    "    train, test, weather, underground_transport, overground_transport, cars_count\n",
    ")\n",
    "\n",
    "y_train = X_train[\"log_bike_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TableVectorizer()\n",
    "model = xgb.XGBRegressor()\n",
    "pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"model\", model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train.drop(columns=[\"bike_count\", \"log_bike_count\"]), y_train)\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_pred, columns=[\"log_bike_count\"]).reset_index().rename(\n",
    "    columns={\"index\": \"Id\"}\n",
    ").to_csv(\"/Users/pierrehaas/bike_counters/predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
