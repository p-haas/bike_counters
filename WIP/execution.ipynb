{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import datetime\n",
    "import io\n",
    "import pickle\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "# Third-Party Imports\n",
    "from flaml import AutoML\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna_integration import OptunaSearchCV\n",
    "import pandas as pd\n",
    "import PyGRF\n",
    "import requests\n",
    "from scipy.stats import mode\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from skrub import DatetimeEncoder, TableVectorizer\n",
    "import xgboost as xgb\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import Sequential, Input\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "# from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the number of threads for inter- and intra-op parallelism\n",
    "# tf.config.threading.set_inter_op_parallelism_threads(12)\n",
    "# tf.config.threading.set_intra_op_parallelism_threads(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import provided data\n",
    "train = pd.read_parquet(\"/Users/pierrehaas/bike_counters/data/train.parquet\")\n",
    "test = pd.read_parquet(\"/Users/pierrehaas/bike_counters/data/final_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additionally sourced data\n",
    "\n",
    "# https://meteo.data.gouv.fr/datasets/donnees-climatologiques-de-base-horaires/\n",
    "weather = pd.read_csv(\n",
    "    \"/Users/pierrehaas/bike_counters/external_data/weather/H_75_previous-2020-2022.csv.gz\",\n",
    "    parse_dates=[\"AAAAMMJJHH\"],\n",
    "    date_format=\"%Y%m%d%H\",\n",
    "    compression=\"gzip\",\n",
    "    sep=\";\",\n",
    ").rename(columns={\"AAAAMMJJHH\": \"date\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public transport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/information/\n",
    "# URLs of the zip files\n",
    "urls = [\n",
    "    \"https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/files/e6bcf4c994951fc086e31db6819a3448/download/\",\n",
    "    \"https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/files/e35b9ec0a183a8f2c7a8537dd43b124c/download/\",\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# File matching pattern\n",
    "pattern = r\"data-rf-202\\d/202\\d_S\\d+_NB_FER\\.txt\"\n",
    "\n",
    "# Process each ZIP file\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            # Get a list of all files in the archive and filter matching files\n",
    "            matching_files = [f for f in z.namelist() if re.match(pattern, f)]\n",
    "\n",
    "            # Read and concatenate the matching files\n",
    "            for file in matching_files:\n",
    "                with z.open(file) as f:\n",
    "                    # Assuming the files are tab-separated and have a \"JOUR\" column\n",
    "                    df = pd.read_csv(f, sep=\"\\t\", parse_dates=[\"JOUR\"], dayfirst=True)\n",
    "                    dfs.append(df)\n",
    "\n",
    "# Combine all DataFrames\n",
    "underground_transport = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "\n",
    "# # https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-surface/information/\n",
    "# # URLs of the zip files\n",
    "urls = [\n",
    "    \"https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-surface/files/41adcbd4216382c232ced4ccbf60187e/download/\",\n",
    "    \"https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-surface/files/68cac32e8717f476905a60006a4dca26/download/\",\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# File matching pattern\n",
    "pattern = r\"data-rs-202\\d/202\\d_T\\d+_NB_SURFACE\\.txt\"\n",
    "\n",
    "# Process each ZIP file\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            # Get a list of all files in the archive and filter matching files\n",
    "            matching_files = [f for f in z.namelist() if re.match(pattern, f)]\n",
    "\n",
    "            # Read and concatenate the matching files\n",
    "            for file in matching_files:\n",
    "                with z.open(file) as f:\n",
    "                    # Assuming the files are tab-separated and have a \"JOUR\" column\n",
    "                    df = pd.read_csv(\n",
    "                        f,\n",
    "                        sep=\"\\t\",\n",
    "                        parse_dates=[\"JOUR\"],\n",
    "                        dayfirst=True,\n",
    "                        encoding=\"latin1\",\n",
    "                    )\n",
    "                    dfs.append(df)\n",
    "\n",
    "# Combine all DataFrames\n",
    "overground_transport = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Car traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs of the zip files\n",
    "urls = [\n",
    "    \"https://parisdata.opendatasoft.com/api/datasets/1.0/comptages-routiers-permanents-historique/attachments/opendata_txt_2020_zip/\",\n",
    "    # \"https://parisdata.opendatasoft.com/api/datasets/1.0/comptages-routiers-permanents-historique/attachments/opendata_txt_2021_zip/\", # This file's compression format is broken, thus I provide a download link below\n",
    "    \"https://www.dropbox.com/scl/fi/sfqzlzpyxcf4yied3yucc/comptage-routier-2021.zip?rlkey=6k6hr3kywl8tvm4ax1qv2nv88&st=ktehiium&dl=1\",\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Process each ZIP file\n",
    "for i, url in enumerate(urls):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            # Process every file in the archive\n",
    "            for file in z.namelist():\n",
    "                # Skip directories and __MACOSX files\n",
    "                if file.endswith(\"/\") or \"__MACOSX\" in file:\n",
    "                    continue\n",
    "                # For the second URL, ensure files are within \"comptage-routier-2021\" directory\n",
    "                if i == 1 and not file.startswith(\"comptage-routier-2021/\"):\n",
    "                    continue\n",
    "                with z.open(file) as f:\n",
    "                    # Assuming the files are semicolon-separated and have a \"t_1h\" column\n",
    "                    df = pd.read_csv(f, sep=\";\", parse_dates=[\"t_1h\"])\n",
    "                    dfs.append(df)\n",
    "\n",
    "# Combine all DataFrames\n",
    "cars_count = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multimodal traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://parisdata.opendatasoft.com/explore/dataset/comptage-multimodal-comptages/information/?disjunctive.label&disjunctive.mode&disjunctive.voie&disjunctive.sens&disjunctive.trajectoire&sort=-t&basemap=jawg.dark&location=13,48.87023,2.34614\n",
    "multimodal_traffic = pd.read_parquet(\n",
    "    \"/Users/pierrehaas/bike_counters/external_data/multimodal_traffic/comptage-multimodal-comptages.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation | Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_encoder(X, col=\"date\"):\n",
    "    \"\"\"\n",
    "    Preprocesses a DataFrame by extracting and encoding various date-related features from a specified date column.\n",
    "    \n",
    "    Parameters:\n",
    "    X (pd.DataFrame): The input DataFrame containing the date column.\n",
    "    col (str): The name of the date column to be processed. Default is \"date\".\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The updated DataFrame with new date-related features.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = X.copy()  # modify a copy of X\n",
    "\n",
    "    # Encode the date information from the DateOfDeparture columns\n",
    "    X[\"year\"] = X[col].dt.year\n",
    "    X[\"quarter\"] = X[col].dt.quarter\n",
    "    X[\"month\"] = X[col].dt.month\n",
    "    X[\"month_scaled\"] = np.cos(2 * np.pi * X[\"month\"] / 12)\n",
    "    X[\"weekofyear\"] = X[col].dt.isocalendar().week\n",
    "    X[\"day\"] = X[col].dt.day\n",
    "    X[\"weekday\"] = X[col].dt.weekday + 1\n",
    "    X[\"hour\"] = X[col].dt.hour\n",
    "    X[\"hour_scaled\"] = np.cos(2 * np.pi * X[\"hour\"] / 24)\n",
    "\n",
    "    # Binary variable indicating weekend or not (1=weekend, 0=weekday)\n",
    "    X[\"is_weekend\"] = (X[\"weekday\"] > 5).astype(int)\n",
    "\n",
    "    # Binary variable indicating bank holiday or not (1=holiday, 0=not holiday)\n",
    "    import holidays\n",
    "\n",
    "    fr_bank_holidays = holidays.FR()  # Get list of FR holidays\n",
    "    X[\"is_bank_holiday\"] = X[col].apply(lambda x: 1 if x in fr_bank_holidays else 0)\n",
    "\n",
    "    X = X.copy()  # modify a copy of X\n",
    "\n",
    "    # Binary variable indicating school holiday or not (1=holiday, 0=not holiday)\n",
    "    # https://www.data.gouv.fr/fr/datasets/vacances-scolaires-par-zones/\n",
    "    fr_school_holidays = pd.read_csv(\n",
    "        \"/Users/pierrehaas/bike_counters/external_data/vacances_scolaires_france.csv\"\n",
    "    )[[\"date\", \"vacances_zone_c\"]]\n",
    "\n",
    "    # Ensure both DataFrames have a consistent datetime format\n",
    "    X[\"date_normalized\"] = pd.to_datetime(X[col]).dt.normalize()\n",
    "    fr_school_holidays[\"date\"] = pd.to_datetime(\n",
    "        fr_school_holidays[\"date\"]\n",
    "    ).dt.normalize()\n",
    "\n",
    "    # Create a dictionary from the holidays dataset for faster lookup\n",
    "    holiday_mapping = dict(\n",
    "        zip(fr_school_holidays[\"date\"], fr_school_holidays[\"vacances_zone_c\"])\n",
    "    )\n",
    "\n",
    "    # Map the normalized date to the holiday column\n",
    "    X[\"is_school_holiday\"] = (\n",
    "        X[\"date_normalized\"].map(holiday_mapping).fillna(0).astype(int)\n",
    "    )\n",
    "\n",
    "    # Drop the normalized date column if not needed\n",
    "    X.drop(columns=[\"date_normalized\"], inplace=True)\n",
    "\n",
    "    # Finally, return the updated DataFrame\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering on train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_processing(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Preprocesses the training and testing datasets by adding various features.\n",
    "\n",
    "    This function includes two main preprocessing steps:\n",
    "    1. Calculating the distance of each station from a reference station.\n",
    "    2. Adding COVID-19 related features such as lockdown and curfew indicators.\n",
    "    3. Clustering the stations based on the mean log_bike_count.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (pd.DataFrame): The training dataset containing features including 'latitude', 'longitude', 'site_name', and 'date'.\n",
    "    X_test (pd.DataFrame): The testing dataset containing features including 'latitude', 'longitude', 'site_name', and 'date'.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the processed training and testing datasets (X_train, X_test) with additional features.\n",
    "    \"\"\"\n",
    "\n",
    "    def dist_station(X):\n",
    "        \"\"\"\n",
    "        Calculate the distance from a specific station to all other stations.\n",
    "\n",
    "        This function computes the Euclidean distance between the coordinates of a specific station\n",
    "        (\"Totem 73 boulevard de Sébastopol\") and all other stations in the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        X (pd.DataFrame): A DataFrame containing the columns 'latitude', 'longitude', and 'site_name'.\n",
    "\n",
    "        Returns:\n",
    "        np.ndarray: An array of distances from the specified station to all other stations.\n",
    "        \"\"\"\n",
    "\n",
    "        lat_long_station = (\n",
    "            X[[\"latitude\", \"longitude\"]]\n",
    "            - mode(\n",
    "                X[X[\"site_name\"] == \"Totem 73 boulevard de Sébastopol\"][\n",
    "                    [\"latitude\", \"longitude\"]\n",
    "                ]\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "        # We calculate the distance the previously identified site to all others\n",
    "        # We call the result the distance to the center since the station is close to the center of Paris\n",
    "        dist_station = np.linalg.norm(\n",
    "            lat_long_station,\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        return dist_station\n",
    "\n",
    "    def covid_features(X):\n",
    "        \"\"\"\n",
    "        Adds COVID-19 related features to the dataset.\n",
    "\n",
    "        This includes a binary variable indicating the presence of a lockdown and curfew periods.\n",
    "        Lockdown and curfew dates are based on historical data from France.\n",
    "\n",
    "        Parameters:\n",
    "        X (pd.DataFrame): The dataset containing a 'date' column.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The dataset with additional COVID-19 related features.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a binary variable indicating the presence of a lockdown\n",
    "        # https://fr.wikipedia.org/wiki/Chronologie_de_la_pand%C3%A9mie_de_Covid-19_en_France\n",
    "        lockdown_dates = [\n",
    "            (\"2020-10-30\", \"2020-12-15\"),\n",
    "            (\"2021-04-03\", \"2021-05-03\"),\n",
    "        ]\n",
    "\n",
    "        X[\"covid_lockdown\"] = 0\n",
    "\n",
    "        for start, end in lockdown_dates:\n",
    "            X.loc[(X[\"date\"] >= start) & (X[\"date\"] < end), \"covid_lockdown\"] = 1\n",
    "\n",
    "        curfew_dates = [\n",
    "            (\"2020-10-17\", \"2020-10-30\", 21, 6),  # 21h-6h\n",
    "            (\"2020-12-16\", \"2021-01-15\", 20, 6),  # 20h-6h\n",
    "            (\"2021-01-15\", \"2021-03-19\", 19, 6),  # 19h-6h\n",
    "            (\"2021-03-20\", \"2021-04-03\", 18, 6),  # 18h-6h\n",
    "            (\"2021-05-03\", \"2021-06-09\", 19, 6),  # 19h-6h\n",
    "            (\"2021-06-09\", \"2021-06-20\", 23, 6),  # 23h-6h\n",
    "        ]\n",
    "\n",
    "        X[\"covid_curfew\"] = 0\n",
    "\n",
    "        for start_date, end_date, start_hour, end_hour in curfew_dates:\n",
    "            X.loc[\n",
    "                (X[\"date\"] >= start_date)\n",
    "                & (X[\"date\"] < end_date)\n",
    "                & (X[\"hour\"] >= start_hour)\n",
    "                & (X[\"hour\"] <= end_hour),\n",
    "                \"covid_curfew\",\n",
    "            ] = 1\n",
    "\n",
    "        return X\n",
    "\n",
    "    X_train, X_test = covid_features(X_train), covid_features(X_test)\n",
    "\n",
    "    X_train[\"dist_to_station\"] = dist_station(X_train)\n",
    "    X_test[\"dist_to_station\"] = dist_station(X_test)\n",
    "\n",
    "    # Group by counter_name and calculate the mean log_bike_count\n",
    "    grouped_train = (\n",
    "        X_train.groupby(\"counter_name\", observed=True)[\"log_bike_count\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Reshape the data for clustering\n",
    "    Y = grouped_train[[\"log_bike_count\"]]\n",
    "\n",
    "    # Apply K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=5)\n",
    "    grouped_train[\"cluster\"] = kmeans.fit_predict(Y)\n",
    "\n",
    "    # Sort clusters by mean log_bike_count and reassign cluster labels\n",
    "    sorted_clusters = (\n",
    "        grouped_train.groupby(\"cluster\")[\"log_bike_count\"].mean().sort_values().index\n",
    "    )\n",
    "    cluster_mapping = {\n",
    "        old_label: new_label for new_label, old_label in enumerate(sorted_clusters)\n",
    "    }\n",
    "    grouped_train[\"cluster\"] = grouped_train[\"cluster\"].map(cluster_mapping)\n",
    "\n",
    "    # Merge the cluster labels back to the original DataFrame\n",
    "    X_train = X_train.merge(\n",
    "        grouped_train[[\"counter_name\", \"cluster\"]], on=\"counter_name\", how=\"left\"\n",
    "    )\n",
    "    X_test = X_test.merge(\n",
    "        grouped_train[[\"counter_name\", \"cluster\"]], on=\"counter_name\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering on weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_processing(X, train_min, train_max, test_min, test_max):\n",
    "    \"\"\"\n",
    "    Preprocesses weather data and applies PCA to extract principal components.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Drops unnecessary columns and groups the data by date.\n",
    "    2. Calculates various weather-related features, including lagged and future values.\n",
    "    3. Adds binary indicators for rain and maximum temperature for each day.\n",
    "    4. Splits the data into training and testing sets based on provided date ranges.\n",
    "    5. Applies PCA to reduce the dimensionality of the weather features.\n",
    "    6. Merges the PCA components with additional weather-related features.\n",
    "\n",
    "    Parameters:\n",
    "    X (pd.DataFrame): The input weather dataset containing various weather-related columns.\n",
    "    train_min (str): The start date for the training set (inclusive).\n",
    "    train_max (str): The end date for the training set (inclusive).\n",
    "    test_min (str): The start date for the testing set (inclusive).\n",
    "    test_max (str): The end date for the testing set (inclusive).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the PCA components and additional weather-related features.\n",
    "    \"\"\"\n",
    "\n",
    "    X_reduced = (\n",
    "        X.drop(columns=[\"NUM_POSTE\", \"NOM_USUEL\", \"LAT\", \"LON\", \"QDXI3S\"])\n",
    "        .groupby(\"date\")\n",
    "        .mean()\n",
    "        .dropna(axis=1, how=\"all\")\n",
    "        .interpolate(method=\"linear\")\n",
    "    )\n",
    "\n",
    "    X_reduced[\"is_rain\"] = (X_reduced[\"RR1\"] > 0).astype(int)\n",
    "\n",
    "    X_reduced[\"q_rain_lag_1h\"] = X_reduced[\"RR1\"].shift(1)\n",
    "    X_reduced[\"t_rain_lag_1h\"] = X_reduced[\"DRR1\"].shift(1)\n",
    "    X_reduced[\"is_rain_lag_1h\"] = X_reduced[\"is_rain\"].shift(1)\n",
    "\n",
    "    X_reduced[\"q_rain_next_1h\"] = X_reduced[\"RR1\"].shift(-1)\n",
    "    X_reduced[\"t_rain_next_1h\"] = X_reduced[\"DRR1\"].shift(-1)\n",
    "    X_reduced[\"is_rain_next_1h\"] = X_reduced[\"is_rain\"].shift(-1)\n",
    "\n",
    "    X_reduced[\"temp_lag_1h\"] = X_reduced[\"T\"].shift(1)\n",
    "    X_reduced[\"temp_next_1h\"] = X_reduced[\"T\"].shift(-1)\n",
    "\n",
    "    X_reduced[\"max_temp\"] = X_reduced.groupby(X_reduced.index.date)[\"T\"].transform(\n",
    "        \"max\"\n",
    "    )\n",
    "    X_reduced[\"will_rain\"] = (\n",
    "        X_reduced.groupby(X_reduced.index.date)[\"RR1\"]\n",
    "        .transform(lambda x: (x > 0).any())\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    weather_features = [\n",
    "        \"RR1\",\n",
    "        \"DRR1\",\n",
    "        \"T\",\n",
    "        \"TNSOL\",\n",
    "        \"TCHAUSSEE\",\n",
    "        \"U\",\n",
    "        \"GLO\",\n",
    "        \"q_rain_lag_1h\",\n",
    "        \"t_rain_lag_1h\",\n",
    "        \"q_rain_next_1h\",\n",
    "        \"t_rain_next_1h\",\n",
    "        \"temp_lag_1h\",\n",
    "        \"temp_next_1h\",\n",
    "        \"max_temp\",\n",
    "    ]\n",
    "\n",
    "    X_reduced_train = X_reduced[\n",
    "        (X_reduced.index >= train_min) & (X_reduced.index <= train_max)\n",
    "    ]\n",
    "\n",
    "    X_reduced_test = X_reduced[\n",
    "        (X_reduced.index >= test_min) & (X_reduced.index <= test_max)\n",
    "    ]\n",
    "\n",
    "    n = 5\n",
    "    pca = PCA(n_components=n)\n",
    "\n",
    "    pca.fit(X_reduced_train[weather_features])\n",
    "\n",
    "    X_pca_train = pca.transform(X_reduced_train[weather_features])\n",
    "    X_pca_test = pca.transform(X_reduced_test[weather_features])\n",
    "\n",
    "    X_pca_train = pd.DataFrame(\n",
    "        X_pca_train,\n",
    "        index=X_reduced_train[weather_features].index,\n",
    "        columns=[\"weather_\" + str(i) for i in range(1, n + 1)],\n",
    "    ).reset_index()\n",
    "\n",
    "    X_pca_test = pd.DataFrame(\n",
    "        X_pca_test,\n",
    "        index=X_reduced_test[weather_features].index,\n",
    "        columns=[\"weather_\" + str(i) for i in range(1, n + 1)],\n",
    "    ).reset_index()\n",
    "\n",
    "    X_pca = pd.concat([X_pca_train, X_pca_test], ignore_index=True)\n",
    "\n",
    "    X_pca = X_pca.merge(\n",
    "        X_reduced[(X_reduced.index >= train_min) & (X_reduced.index <= test_max)][\n",
    "            [\"is_rain\", \"is_rain_lag_1h\", \"is_rain_next_1h\", \"will_rain\"]\n",
    "        ],\n",
    "        on=\"date\",\n",
    "    )\n",
    "\n",
    "    return X_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing of public transport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transport_processing(X1, X2, train_min=\"2020-09-01\", test_max=\"2021-10-18\"):\n",
    "    \"\"\"\n",
    "    Preprocesses transport data by aggregating daily validations and encoding date-related features.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Aggregates daily validation counts from two datasets.\n",
    "    2. Combines the aggregated counts from both datasets.\n",
    "    3. Encodes various date-related features using the `date_encoder` function.\n",
    "    4. Filters the data based on the specified date range.\n",
    "\n",
    "    Parameters:\n",
    "    X1 (pd.DataFrame): The first dataset containing transport data with columns 'JOUR' (date) and 'NB_VALD' (number of validations).\n",
    "    X2 (pd.DataFrame): The second dataset containing transport data with columns 'JOUR' (date) and 'NB_VALD' (number of validations).\n",
    "    train_min (str): The start date for the training set (inclusive). Default is \"2020-09-01\".\n",
    "    test_max (str): The end date for the testing set (inclusive). Default is \"2021-10-18\".\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the aggregated and encoded transport data within the specified date range.\n",
    "    \"\"\"\n",
    "\n",
    "    daily_X1 = X1.groupby(\"JOUR\")[\"NB_VALD\"].sum()\n",
    "    daily_X2 = X2.groupby(\"JOUR\")[\"NB_VALD\"].sum()\n",
    "\n",
    "    X = (daily_X1 + daily_X2).reset_index()\n",
    "\n",
    "    X_reduced = date_encoder(X, col=\"JOUR\")[\n",
    "        (X[\"JOUR\"] >= train_min) & (X[\"JOUR\"] <= test_max)\n",
    "    ]\n",
    "\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing of car traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def car_traffic_processing(X, train_min, test_max):\n",
    "    \"\"\"\n",
    "    Preprocesses car traffic data by aggregating hourly traffic counts and calculating daily cumulative sums.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Aggregates hourly traffic counts.\n",
    "    2. Calculates the cumulative sum of traffic counts for each day.\n",
    "    3. Filters the data based on the specified date range.\n",
    "\n",
    "    Parameters:\n",
    "    X (pd.DataFrame): The input dataset containing car traffic data with columns 't_1h' (timestamp) and 'q' (traffic count).\n",
    "    train_min (str): The start date for the training set (inclusive).\n",
    "    test_max (str): The end date for the testing set (inclusive).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the aggregated and processed car traffic data within the specified date range.\n",
    "    \"\"\"\n",
    "    X_hourly = X.groupby(\"t_1h\")[\"q\"].sum().reset_index()\n",
    "\n",
    "    # Group by day and calculate the cumulative sum of 'q' for each day\n",
    "    X_hourly[\"daily_cumsum\"] = X_hourly.groupby(X_hourly[\"t_1h\"].dt.to_period(\"d\"))[\n",
    "        \"q\"\n",
    "    ].cumsum()\n",
    "\n",
    "    return X_hourly[(X_hourly[\"t_1h\"] >= train_min) & (X_hourly[\"t_1h\"] <= test_max)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing of multimodal traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mm_traffic_processing(X, train_min, test_max):\n",
    "    \"\"\"\n",
    "    Preprocesses multimodal traffic data by encoding dates, removing bike counts, and calculating cumulative sums.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Encodes dates appropriately by removing timezone information.\n",
    "    2. Removes bike counts to avoid feeding the model with the target variable.\n",
    "    3. Aggregates hourly traffic counts for each mode of transport.\n",
    "    4. Calculates the cumulative sum of traffic counts for each vehicle type every day.\n",
    "    5. Combines the original hourly counts with the cumulative sums.\n",
    "    6. Filters the data based on the specified date range and fills missing values with zero.\n",
    "\n",
    "    Parameters:\n",
    "    X (pd.DataFrame): The input dataset containing traffic data with columns 't' (timestamp), 'mode' (mode of transport), and 'nb_usagers' (number of users).\n",
    "    train_min (str): The start date for the training set (inclusive).\n",
    "    test_max (str): The end date for the testing set (inclusive).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the processed traffic data within the specified date range, with additional features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Encode dates appropriately like train and test sets\n",
    "    X[\"date\"] = X[\"t\"].dt.tz_localize(None)\n",
    "\n",
    "    # Remove bike count to avoid feeding the model with the target variable\n",
    "    mask = (X[\"mode\"] == \"Trottinettes + vélos\") | (X[\"mode\"] == \"Vélos\")\n",
    "    hourly_vehicle_count = (\n",
    "        X[~mask]\n",
    "        .groupby([\"date\", \"mode\"])[\"nb_usagers\"]\n",
    "        .sum()\n",
    "        .unstack()\n",
    "        .drop(columns=\"van\")\n",
    "    )\n",
    "\n",
    "    # Group by day and calculate the cumulative sum of 'nb_usagers' for each vehicle type every day\n",
    "    daily_cumsum_vh_type = (\n",
    "        hourly_vehicle_count.groupby(hourly_vehicle_count.index.date)\n",
    "        .cumsum()\n",
    "        .rename(columns=lambda x: x + \"_cumsum\")\n",
    "    )\n",
    "\n",
    "    mm_traffic_features = pd.concat(\n",
    "        [\n",
    "            hourly_vehicle_count,\n",
    "            daily_cumsum_vh_type,\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).reset_index()\n",
    "\n",
    "    return mm_traffic_features[\n",
    "        (mm_traffic_features[\"date\"] >= train_min)\n",
    "        & (mm_traffic_features[\"date\"] <= test_max)\n",
    "    ].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_engineered(\n",
    "    df_train,\n",
    "    df_test,\n",
    "    weather,\n",
    "    underground_transport,\n",
    "    overground_transport,\n",
    "    cars_count,\n",
    "    mm_traffic_count,\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocesses and merges various datasets to create engineered features for training and testing.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Encodes date-related features in the training and testing datasets.\n",
    "    2. Merges weather data, public transport data, car traffic data, and multimodal traffic data with the training and testing datasets.\n",
    "\n",
    "    Parameters:\n",
    "    df_train (pd.DataFrame): The training dataset containing a 'date' column.\n",
    "    df_test (pd.DataFrame): The testing dataset containing a 'date' column.\n",
    "    weather (pd.DataFrame): The weather dataset containing weather-related features and a 'date' column.\n",
    "    underground_transport (pd.DataFrame): The underground transport dataset containing transport data with columns 'year', 'month', 'day', and 'NB_VALD'.\n",
    "    overground_transport (pd.DataFrame): The overground transport dataset containing transport data with columns 'year', 'month', 'day', and 'NB_VALD'.\n",
    "    cars_count (pd.DataFrame): The car traffic dataset containing traffic data with columns 't_1h' (timestamp) and 'q' (traffic count).\n",
    "    mm_traffic_count (pd.DataFrame): The multimodal traffic dataset containing traffic data with columns 'date', 'mode', and 'nb_usagers'.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the processed training and testing datasets (df_train, df_test) with merged features.\n",
    "    \"\"\"\n",
    "\n",
    "    def merging_data(data, weather, public_transport, car_traffic, mm_traffic):\n",
    "        \"\"\"\n",
    "        Merges weather, public transport, car traffic, and multimodal traffic data with the main dataset.\n",
    "\n",
    "        Parameters:\n",
    "        data (pd.DataFrame): The main dataset to merge with other datasets.\n",
    "        weather (pd.DataFrame): The weather dataset containing weather-related features and a 'date' column.\n",
    "        public_transport (pd.DataFrame): The public transport dataset containing transport data with columns 'year', 'month', 'day', and 'NB_VALD'.\n",
    "        car_traffic (pd.DataFrame): The car traffic dataset containing traffic data with columns 't_1h' (timestamp) and 'q' (traffic count).\n",
    "        mm_traffic (pd.DataFrame): The multimodal traffic dataset containing traffic data with columns 'date', 'mode', and 'nb_usagers'.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The merged dataset with additional features.\n",
    "        \"\"\"\n",
    "\n",
    "        # Merge weather data\n",
    "        data = data.merge(weather, on=\"date\", how=\"left\")\n",
    "\n",
    "        # Merge public transport data\n",
    "        data = data.merge(\n",
    "            public_transport[[\"year\", \"month\", \"day\", \"NB_VALD\"]],\n",
    "            on=[\"year\", \"month\", \"day\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "        # Merge car traffic data\n",
    "        data = (\n",
    "            data.merge(car_traffic, left_on=\"date\", right_on=\"t_1h\", how=\"left\")\n",
    "            .drop(columns=[\"t_1h\"])\n",
    "            .dropna()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        data = data.merge(mm_traffic, on=\"date\", how=\"left\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    train_min, train_max = df_train[\"date\"].min(), df_train[\"date\"].max()\n",
    "    test_min, test_max = df_test[\"date\"].min(), df_test[\"date\"].max()\n",
    "\n",
    "    # Encoding the date\n",
    "    df_train, df_test = date_encoder(df_train), date_encoder(df_test)\n",
    "\n",
    "    # Processing the data\n",
    "    df_train, df_test = train_test_processing(df_train, df_test)\n",
    "\n",
    "    # Processing weather data\n",
    "    weather_processed = weather_processing(\n",
    "        weather, train_min, train_max, test_min, test_max\n",
    "    )\n",
    "\n",
    "    # Processing transport data\n",
    "    transport_processed = transport_processing(\n",
    "        underground_transport, overground_transport\n",
    "    )\n",
    "\n",
    "    # Processing car traffic data\n",
    "    new_index = df_train[\"date\"].unique().tolist() + df_test[\"date\"].unique().tolist()\n",
    "    car_traffic_processed = (\n",
    "        car_traffic_processing(cars_count, train_min, test_max)\n",
    "        .set_index(\"t_1h\")\n",
    "        .reindex(new_index)\n",
    "        .interpolate(method=\"linear\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Processing multimodal traffic data\n",
    "    mm_traffic_processed = mm_traffic_processing(mm_traffic_count, train_min, test_max)\n",
    "\n",
    "    # Merging the data\n",
    "    df_train = merging_data(\n",
    "        df_train,\n",
    "        weather_processed,\n",
    "        transport_processed,\n",
    "        car_traffic_processed,\n",
    "        mm_traffic_processed,\n",
    "    )\n",
    "\n",
    "    df_test = merging_data(\n",
    "        df_test,\n",
    "        weather_processed,\n",
    "        transport_processed,\n",
    "        car_traffic_processed,\n",
    "        mm_traffic_processed,\n",
    "    )\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation | Data cleaning | Pipeline creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, X_test = data_engineered(\n",
    "    train,\n",
    "    test,\n",
    "    weather,\n",
    "    underground_transport,\n",
    "    overground_transport,\n",
    "    cars_count,\n",
    "    multimodal_traffic,\n",
    ")\n",
    "\n",
    "df_train[\"total_seconds\"] = DatetimeEncoder().fit_transform(df_train[\"date\"])[\"date_total_seconds\"]\n",
    "X_test[\"total_seconds\"] = DatetimeEncoder().fit_transform(X_test[\"date\"])[\"date_total_seconds\"]\n",
    "\n",
    "df_train.sort_values([\"date\", \"counter_id\"], inplace=True)\n",
    "\n",
    "y_train = df_train[\"log_bike_count\"]\n",
    "\n",
    "X_train = df_train.drop(\n",
    "    columns=[\n",
    "        \"date\",\n",
    "        \"counter_name\",\n",
    "        \"site_name\",\n",
    "        \"counter_installation_date\",\n",
    "        \"bike_count\",\n",
    "        \"log_bike_count\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"2 roues motorisées\",\n",
    "    \"Autobus et autocars\",\n",
    "    \"Trottinettes\",\n",
    "    \"Véhicules lourds > 3,5t\",\n",
    "    \"Véhicules légers < 3,5t\",\n",
    "    \"2 roues motorisées_cumsum\",\n",
    "    \"Autobus et autocars_cumsum\",\n",
    "    \"Trottinettes_cumsum\",\n",
    "    \"Véhicules lourds > 3,5t_cumsum\",\n",
    "    \"Véhicules légers < 3,5t_cumsum\",\n",
    "]\n",
    "\n",
    "X_train[cols] = X_train[cols].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing specific observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = (\n",
    "#     X_train.groupby(\"counter_id\", observed=True)[\"bike_count\"]\n",
    "#     .rolling(window=24)\n",
    "#     .mean()\n",
    "#     .reset_index(level=0, drop=True)\n",
    "#     != 0\n",
    "# )\n",
    "\n",
    "# X_train_1 = X_train[mask.sort_index()]\n",
    "# y_train_1 = X_train_1[\"log_bike_count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't seem to be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = [\n",
    "    \"total_seconds\",\n",
    "    \"weather_1\",\n",
    "    \"weather_2\",\n",
    "    \"weather_3\",\n",
    "    \"weather_4\",\n",
    "    \"weather_5\",\n",
    "    \"weather_6\",\n",
    "    \"weather_7\",\n",
    "    \"weather_8\",\n",
    "    \"weather_9\",\n",
    "    \"weather_10\",\n",
    "    \"NB_VALD\",\n",
    "    \"q\",\n",
    "    \"daily_cumsum\",\n",
    "    \"2 roues motorisées\",\n",
    "    \"Autobus et autocars\",\n",
    "    \"Trottinettes\",\n",
    "    \"Véhicules lourds > 3,5t\",\n",
    "    \"Véhicules légers < 3,5t\",\n",
    "    \"2 roues motorisées_cumsum\",\n",
    "    \"Autobus et autocars_cumsum\",\n",
    "    \"Trottinettes_cumsum\",\n",
    "    \"Véhicules lourds > 3,5t_cumsum\",\n",
    "    \"Véhicules légers < 3,5t_cumsum\",\n",
    "]\n",
    "\n",
    "cols_to_label_encode = [\n",
    "    \"counter_id\",\n",
    "    \"site_id\",\n",
    "    \"coordinates\",\n",
    "    \"counter_technical_id\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor = ColumnTransformer(\n",
    "#     [\n",
    "#         (\"scaler\", StandardScaler(), cols_to_scale),\n",
    "#         (\"label_encoder\", OrdinalEncoder(), cols_to_label_encode),\n",
    "#     ],\n",
    "#     remainder=\"passthrough\",\n",
    "# )\n",
    "\n",
    "# preprocessor.fit(X_train)\n",
    "# X_train_enc = preprocessor.transform(X_train)\n",
    "# X_test_enc = preprocessor.transform(X_test)\n",
    "\n",
    "# # Convert the data types of the numpy ndarrays\n",
    "# X_train_enc = X_train_enc.astype(float)\n",
    "# X_test_enc = X_test_enc.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler(), cols_to_scale),\n",
    "        (\"label_encoder\", OrdinalEncoder(), cols_to_label_encode),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "preprocessor.fit(X_train)\n",
    "X_train_enc = preprocessor.transform(X_train)\n",
    "X_test_enc = preprocessor.transform(X_test)\n",
    "\n",
    "# Get feature names from the transformers\n",
    "scaler_feature_names = [f\"{col}_scaled\" for col in cols_to_scale]\n",
    "label_encoder_feature_names = [f\"{col}_encoded\" for col in cols_to_label_encode]\n",
    "passthrough_feature_names = [col for col in X_train.columns if col not in cols_to_scale + cols_to_label_encode]\n",
    "\n",
    "# Combine all feature names\n",
    "all_feature_names = scaler_feature_names + label_encoder_feature_names + passthrough_feature_names\n",
    "\n",
    "# Convert the numpy ndarray to a pandas DataFrame\n",
    "X_train_enc_df = pd.DataFrame(X_train_enc, columns=all_feature_names)\n",
    "X_test_enc_df = pd.DataFrame(X_test_enc, columns=all_feature_names)\n",
    "\n",
    "# Convert data types explicitly\n",
    "X_train_enc_df = X_train_enc_df.apply(pd.to_numeric, errors='coerce')\n",
    "X_test_enc_df = X_test_enc_df.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor = TableVectorizer(\n",
    "#     # specific_transformers=[(\"passthrough\", [\"date\"])]\n",
    "# )\n",
    "# preprocessor.fit(X_train)\n",
    "# X_train_enc = preprocessor.transform(X_train).drop(\n",
    "#     columns=[\"date_year\", \"date_month\", \"date_day\", \"date_hour\"]\n",
    "# )\n",
    "# X_test_enc = preprocessor.transform(X_test).drop(\n",
    "#     columns=[\"date_year\", \"date_month\", \"date_day\", \"date_hour\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automl = AutoML()\n",
    "# automl.fit(\n",
    "#     X_train_enc,\n",
    "#     y_train,\n",
    "#     task=\"regression\",\n",
    "#     metric=\"rmse\",\n",
    "#     time_budget=-1,\n",
    "#     eval_method=\"cv\",\n",
    "#     n_splits=3,\n",
    "#     split_type=TimeSeriesSplit(),\n",
    "#     estimator_list=[\n",
    "#         \"histgb\",\n",
    "#         \"catboost\",\n",
    "#         \"kneighbor\",\n",
    "#         \"extra_tree\",\n",
    "#         \"rf\",\n",
    "#         \"xgboost\",\n",
    "#         \"xgb_limitdepth\",\n",
    "#         \"sgd\",\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# # Save the model\n",
    "# with open(\"/Users/pierrehaas/Desktop/automl_all_reg_models.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(automl, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # At prediction time\n",
    "# with open(\"/Users/pierrehaas/Desktop/automl_all_reg_models.pkl\", \"rb\") as f:\n",
    "#     automl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(automl.best_estimator)\n",
    "# print(automl.best_config)\n",
    "# print(automl.best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(automl.best_loss_per_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(automl.best_config_per_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna_parameters = {\n",
    "#     \"n_estimators\": optuna.distributions.IntDistribution(50, 500),\n",
    "#     \"max_depth\": optuna.distributions.IntDistribution(5, 50),\n",
    "#     \"min_samples_split\": optuna.distributions.IntDistribution(2, 50),\n",
    "#     \"min_samples_leaf\": optuna.distributions.IntDistribution(1, 50),\n",
    "#     \"max_features\": optuna.distributions.CategoricalDistribution(\n",
    "#         [\"sqrt\", \"log2\", None]\n",
    "#     ),\n",
    "# }\n",
    "\n",
    "# model = ExtraTreesRegressor()\n",
    "\n",
    "# optimiziming_etr = OptunaSearchCV(\n",
    "#     model,\n",
    "#     optuna_parameters,\n",
    "#     n_jobs=-1,\n",
    "#     cv=TimeSeriesSplit(n_splits=2),\n",
    "#     scoring=\"neg_root_mean_squared_error\",\n",
    "#     n_trials=150,\n",
    "# )\n",
    "\n",
    "# optimiziming_etr.fit(X_train_enc, y_train)\n",
    "\n",
    "# y_hat = optimiziming_etr.predict(X_test_enc)\n",
    "\n",
    "# print(optimiziming_etr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the full pipeline with the best parameters\n",
    "# joblib.dump(optimiziming_etr.best_estimator_, \"/Users/pierrehaas/bike_counters/WIP/best_etr.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etr_optimized = joblib.load(\"/Users/pierrehaas/bike_counters/WIP/best_etr.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from tensorflow.keras import mixed_precision\n",
    "# from tensorflow.keras.regularizers import l2\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Enable mixed precision\n",
    "# mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# # Verify that TensorFlow is using the GPU\n",
    "# print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices(\"GPU\")))\n",
    "\n",
    "# X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(\n",
    "#     X_train_enc, y_train, test_size=0.1, shuffle=False\n",
    "# )\n",
    "\n",
    "# split_index = int(len(X_train_s) * 0.8)\n",
    "# X_train_s, X_train_val = X_train_s[:split_index], X_train_s[split_index:]\n",
    "# y_train_s, y_train_val = y_train_s[:split_index], y_train_s[split_index:]\n",
    "\n",
    "# # Suppose each sample is already [T, F] after reshaping\n",
    "# T = 10\n",
    "# F = X_train_s.shape[1] // T\n",
    "# X_train_s = X_train_s.reshape((X_train_s.shape[0], T, F))\n",
    "# X_train_val = X_train_val.reshape((X_train_val.shape[0], T, F))\n",
    "# X_test_s = X_test_s.reshape((X_test_s.shape[0], T, F))\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Input(shape=(T, F)))  # Define the input shape here\n",
    "# model.add(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.001)))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(LSTM(64))\n",
    "# model.add(Dense(1, dtype=\"float32\"))  # Ensure the output layer uses float32\n",
    "\n",
    "# es = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "# mc = ModelCheckpoint(\n",
    "#     \"/Users/pierrehaas/bike_counters/WIP/best_model.keras\",\n",
    "#     monitor=\"val_loss\",\n",
    "#     save_best_only=True,\n",
    "# )\n",
    "\n",
    "# model.compile(loss=\"mean_squared_error\", optimizer=Adam(learning_rate=0.0005))\n",
    "# model.fit(\n",
    "#     X_train_s,\n",
    "#     y_train_s,\n",
    "#     epochs=50,\n",
    "#     batch_size=32,\n",
    "#     validation_data=(X_train_val, y_train_val),\n",
    "#     callbacks=[es, mc],\n",
    "#     verbose=1,\n",
    "# )\n",
    "\n",
    "# test_loss = model.evaluate(X_test_s, y_test_s, verbose=1)\n",
    "# print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = model.predict(X_test_enc.reshape((X_test_enc.shape[0], T, F)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesRegressor()\n",
    "\n",
    "model.fit(\n",
    "    X_train_enc,\n",
    "    y_train,\n",
    ")\n",
    "\n",
    "y_pred = model.predict(X_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get feature importances\n",
    "# importances = etr_optimized.feature_importances_\n",
    "\n",
    "# # Display feature contributions\n",
    "# feature_importance_df = pd.DataFrame(\n",
    "#     {\"Feature\": X_train_enc.columns.values, \"Importance\": importances}\n",
    "# ).sort_values(by=\"Importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15, 15))\n",
    "# plt.barh(\n",
    "#     feature_importance_df[feature_importance_df[\"Importance\"] > 0.001][\"Feature\"],\n",
    "#     feature_importance_df[feature_importance_df[\"Importance\"] > 0.001][\"Importance\"],\n",
    "# )\n",
    "# plt.xlabel(\"Importance\")\n",
    "# plt.ylabel(\"Feature\")\n",
    "# plt.title(\"Feature Importances\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographical RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/libpysal/weights/distance.py:153: UserWarning: The weights matrix is not fully connected: \n",
      " There are 30 disconnected components.\n",
      "  W.__init__(self, neighbors, id_order=ids, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "coords = [\"latitude\", \"longitude\"]\n",
    "bandwidth, local_weight, p_value = PyGRF.search_bw_lw_ISA(\n",
    "    y_train, X_train_enc_df[coords]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygrf = PyGRF.PyGRFBuilder(\n",
    "    band_width=bandwidth,\n",
    "    train_weighted=True,\n",
    "    predict_weighted=True,\n",
    "    bootstrap=False,\n",
    "    resampled=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "pygrf.fit(X_train_enc_df.drop(columns=coords), y_train, X_train_enc_df[coords])\n",
    "\n",
    "predict_combined, predict_global, predict_local = pygrf.predict(\n",
    "    X_test_enc_df.drop(columns=coords), X_test_enc_df[coords], local_weight=local_weight\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(predict_combined, columns=[\"log_bike_count\"]).reset_index().rename(\n",
    "    columns={\"index\": \"Id\"}\n",
    ").to_csv(\"/Users/pierrehaas/bike_counters/predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
